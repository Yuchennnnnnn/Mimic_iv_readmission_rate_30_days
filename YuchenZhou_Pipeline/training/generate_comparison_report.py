#!/usr/bin/env python3
"""
Generate Model Comparison Report
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Read results
df = pd.read_csv('reports/metrics.csv')

# Remove duplicates
df = df.drop_duplicates(subset=['model'])

print("="*80)
print("Model Performance Comparison Report")
print("="*80)
print("\nUsing features: 18 selected features (mapped from LASSO's 48 features)")
print("Dataset: MIMIC-IV cleaned_data.csv")
print("Sample size: 205,980 (train: 164,784 | test: 41,196)")
print("Readmission rate: 26.72%\n")

# Formatted output
print("-"*80)
print(f"{'Model':<20} {'ROC-AUC':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}")
print("-"*80)

for _, row in df.iterrows():
    print(f"{row['model']:<20} {row['roc_auc']:<10.4f} {row['accuracy']:<10.4f} "
          f"{row['precision']:<10.4f} {row['recall']:<10.4f} {row['f1']:<10.4f}")

print("-"*80)

# Find best models
best_auc = df.loc[df['roc_auc'].idxmax()]
best_f1 = df.loc[df['f1'].idxmax()]
best_recall = df.loc[df['recall'].idxmax()]

print(f"\nðŸ† Best ROC-AUC: {best_auc['model']} ({best_auc['roc_auc']:.4f})")
print(f"ðŸ† Best F1-Score: {best_f1['model']} ({best_f1['f1']:.4f})")
print(f"ðŸ† Best Recall: {best_recall['model']} ({best_recall['recall']:.4f})")

# Create comparison plots
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Model Performance Comparison - 30-Day Readmission Prediction\nUsing 18 LASSO Selected Features', 
             fontsize=16, fontweight='bold')

metrics = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1', 'pr_auc']
titles = ['ROC-AUC', 'Accuracy', 'Precision', 'Recall (Sensitivity)', 'F1-Score', 'PR-AUC']
colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12', '#1abc9c']

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    ax = axes[idx // 3, idx % 3]
    
    # Draw bar chart
    bars = ax.bar(df['model'], df[metric], color=color, alpha=0.7, edgecolor='black')
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontweight='bold')
    
    ax.set_ylabel(title, fontsize=11, fontweight='bold')
    ax.set_xlabel('Model', fontsize=10)
    ax.set_ylim(0, max(df[metric]) * 1.15)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_title(title, fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('reports/model_comparison.png', dpi=300, bbox_inches='tight')
print(f"\nðŸ“Š Comparison plot saved: reports/model_comparison.png")

# Confusion matrix comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
fig.suptitle('Confusion Matrix Comparison', fontsize=14, fontweight='bold')

for idx, (_, row) in enumerate(df.iterrows()):
    ax = axes[idx]
    
    # Build confusion matrix
    cm = np.array([
        [row['true_negatives'], row['false_positives']],
        [row['false_negatives'], row['true_positives']]
    ])
    
    # Draw heatmap
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Pred: No', 'Pred: Yes'],
                yticklabels=['True: No', 'True: Yes'],
                ax=ax, cbar=False, square=True)
    
    ax.set_title(f'{row["model"]}\n'
                f'Acc: {row["accuracy"]:.3f} | F1: {row["f1"]:.3f}',
                fontweight='bold')

plt.tight_layout()
plt.savefig('reports/confusion_matrix_comparison.png', dpi=300, bbox_inches='tight')
print(f"ðŸ“Š Confusion matrix comparison saved: reports/confusion_matrix_comparison.png")

# Generate Markdown report
md_report = f"""# Model Training Results Report

**Training Time**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Dataset**: MIMIC-IV cleaned_data.csv  
**Number of Features**: 18 (mapped from LASSO's 48 One-Hot features)  
**Training Samples**: 164,784  
**Test Samples**: 41,196  
**Readmission Rate**: 26.72%  

---

## ðŸ“Š Model Performance Comparison

| Model | ROC-AUC | PR-AUC | Accuracy | Precision | Recall | F1-Score |
|------|---------|--------|----------|-----------|--------|----------|
"""

for _, row in df.iterrows():
    md_report += f"| **{row['model']}** | {row['roc_auc']:.4f} | {row['pr_auc']:.4f} | "
    md_report += f"{row['accuracy']:.4f} | {row['precision']:.4f} | "
    md_report += f"{row['recall']:.4f} | {row['f1']:.4f} |\n"

md_report += f"""
---

## ðŸ† Best Models

- **Best ROC-AUC**: {best_auc['model']} ({best_auc['roc_auc']:.4f})
- **Best F1-Score**: {best_f1['model']} ({best_f1['f1']:.4f})
- **Best Recall**: {best_recall['model']} ({best_recall['recall']:.4f})

---

## ðŸ’¡ Key Findings

1. **XGBoost performs best**: ROC-AUC reaches 0.7029, outperforms other models on all metrics
2. **Recall vs Precision tradeoff**: 
   - XGBoost: Highest recall (68.46%), good for capturing more readmission patients
   - Random Forest: More balanced precision (39.15%)
3. **Feature selection works well**: Using only 18 features achieves 0.70+ AUC

---

## ðŸ“ˆ Detailed Metrics

### Logistic Regression
- ROC-AUC: {df[df['model']=='LR']['roc_auc'].values[0]:.4f}
- Advantages: Fast training, good interpretability
- Use case: Scenarios requiring quick deployment and explanation

### Random Forest  
- ROC-AUC: {df[df['model']=='RF']['roc_auc'].values[0]:.4f}
- Advantages: Automatic handling of non-linear relationships, feature importance visualization
- Use case: When feature importance analysis is needed

### XGBoost â­
- ROC-AUC: {df[df['model']=='XGB']['roc_auc'].values[0]:.4f}
- Advantages: Best performance, handles complex patterns
- Use case: First choice for production environment

---

## ðŸ“ File Locations

- Models: `artifacts/*.pkl`
- Predictions: `reports/predictions_*.csv`
- Visualizations: `reports/*.png`
- Detailed metrics: `reports/metrics.csv`

---

## ðŸ”§ Next Steps

1. **Hyperparameter tuning**: Use GridSearch to optimize XGBoost
2. **Feature engineering**: Try adding more LASSO features (top_n: 100)
3. **Ensemble learning**: Combine predictions from multiple models
4. **Deep learning**: Train LSTM and Transformer models
5. **Model interpretation**: Use SHAP to analyze feature importance
"""

with open('reports/MODEL_COMPARISON_REPORT.md', 'w') as f:
    f.write(md_report)

print(f"ðŸ“„ Markdown report saved: reports/MODEL_COMPARISON_REPORT.md")

print("\n" + "="*80)
print("âœ… Report generation completed!")
print("="*80)
