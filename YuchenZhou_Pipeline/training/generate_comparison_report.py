#!/usr/bin/env python3
"""
ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# è¯»å–ç»“æœ
df = pd.read_csv('reports/metrics.csv')

# å»é‡ï¼ˆæœ‰é‡å¤è¡Œï¼‰
df = df.drop_duplicates(subset=['model'])

print("="*80)
print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
print("="*80)
print("\nä½¿ç”¨ç‰¹å¾: 18ä¸ªç²¾é€‰ç‰¹å¾ (ä»LASSOçš„48ä¸ªç‰¹å¾æ˜ å°„è€Œæ¥)")
print("æ•°æ®é›†: MIMIC-IV cleaned_data.csv")
print("æ ·æœ¬æ•°: 205,980 (è®­ç»ƒ: 164,784 | æµ‹è¯•: 41,196)")
print("å†å…¥é™¢ç‡: 26.72%\n")

# æ ¼å¼åŒ–è¾“å‡º
print("-"*80)
print(f"{'æ¨¡å‹':<20} {'ROC-AUC':<10} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10}")
print("-"*80)

for _, row in df.iterrows():
    print(f"{row['model']:<20} {row['roc_auc']:<10.4f} {row['accuracy']:<10.4f} "
          f"{row['precision']:<10.4f} {row['recall']:<10.4f} {row['f1']:<10.4f}")

print("-"*80)

# æ‰¾å‡ºæœ€ä½³æ¨¡å‹
best_auc = df.loc[df['roc_auc'].idxmax()]
best_f1 = df.loc[df['f1'].idxmax()]
best_recall = df.loc[df['recall'].idxmax()]

print(f"\nğŸ† æœ€ä½³ROC-AUC: {best_auc['model']} ({best_auc['roc_auc']:.4f})")
print(f"ğŸ† æœ€ä½³F1-Score: {best_f1['model']} ({best_f1['f1']:.4f})")
print(f"ğŸ† æœ€ä½³Recall: {best_recall['model']} ({best_recall['recall']:.4f})")

# åˆ›å»ºå¯¹æ¯”å›¾
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('æ¨¡å‹æ€§èƒ½å¯¹æ¯” - 30å¤©å†å…¥é™¢é¢„æµ‹\nä½¿ç”¨18ä¸ªLASSOç²¾é€‰ç‰¹å¾', 
             fontsize=16, fontweight='bold')

metrics = ['roc_auc', 'accuracy', 'precision', 'recall', 'f1', 'pr_auc']
titles = ['ROC-AUC', 'Accuracy', 'Precision', 'Recall (Sensitivity)', 'F1-Score', 'PR-AUC']
colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12', '#1abc9c']

for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
    ax = axes[idx // 3, idx % 3]
    
    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    bars = ax.bar(df['model'], df[metric], color=color, alpha=0.7, edgecolor='black')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontweight='bold')
    
    ax.set_ylabel(title, fontsize=11, fontweight='bold')
    ax.set_xlabel('Model', fontsize=10)
    ax.set_ylim(0, max(df[metric]) * 1.15)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_title(title, fontsize=12, fontweight='bold')

plt.tight_layout()
plt.savefig('reports/model_comparison.png', dpi=300, bbox_inches='tight')
print(f"\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜: reports/model_comparison.png")

# æ··æ·†çŸ©é˜µå¯¹æ¯”
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
fig.suptitle('æ··æ·†çŸ©é˜µå¯¹æ¯”', fontsize=14, fontweight='bold')

for idx, (_, row) in enumerate(df.iterrows()):
    ax = axes[idx]
    
    # æ„å»ºæ··æ·†çŸ©é˜µ
    cm = np.array([
        [row['true_negatives'], row['false_positives']],
        [row['false_negatives'], row['true_positives']]
    ])
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Pred: No', 'Pred: Yes'],
                yticklabels=['True: No', 'True: Yes'],
                ax=ax, cbar=False, square=True)
    
    ax.set_title(f'{row["model"]}\n'
                f'Acc: {row["accuracy"]:.3f} | F1: {row["f1"]:.3f}',
                fontweight='bold')

plt.tight_layout()
plt.savefig('reports/confusion_matrix_comparison.png', dpi=300, bbox_inches='tight')
print(f"ğŸ“Š æ··æ·†çŸ©é˜µå¯¹æ¯”å·²ä¿å­˜: reports/confusion_matrix_comparison.png")

# ç”ŸæˆMarkdownæŠ¥å‘Š
md_report = f"""# æ¨¡å‹è®­ç»ƒç»“æœæŠ¥å‘Š

**è®­ç»ƒæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}  
**æ•°æ®é›†**: MIMIC-IV cleaned_data.csv  
**ç‰¹å¾æ•°**: 18ä¸ª (ä»LASSOçš„48ä¸ªOne-Hotç‰¹å¾æ˜ å°„è€Œæ¥)  
**è®­ç»ƒæ ·æœ¬**: 164,784  
**æµ‹è¯•æ ·æœ¬**: 41,196  
**å†å…¥é™¢ç‡**: 26.72%  

---

## ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | ROC-AUC | PR-AUC | Accuracy | Precision | Recall | F1-Score |
|------|---------|--------|----------|-----------|--------|----------|
"""

for _, row in df.iterrows():
    md_report += f"| **{row['model']}** | {row['roc_auc']:.4f} | {row['pr_auc']:.4f} | "
    md_report += f"{row['accuracy']:.4f} | {row['precision']:.4f} | "
    md_report += f"{row['recall']:.4f} | {row['f1']:.4f} |\n"

md_report += f"""
---

## ğŸ† æœ€ä½³æ¨¡å‹

- **æœ€ä½³ROC-AUC**: {best_auc['model']} ({best_auc['roc_auc']:.4f})
- **æœ€ä½³F1-Score**: {best_f1['model']} ({best_f1['f1']:.4f})
- **æœ€ä½³Recall**: {best_recall['model']} ({best_recall['recall']:.4f})

---

## ğŸ’¡ å…³é”®å‘ç°

1. **XGBoostè¡¨ç°æœ€ä½³**: ROC-AUCè¾¾åˆ°0.7029ï¼Œåœ¨æ‰€æœ‰æŒ‡æ ‡ä¸Šéƒ½ä¼˜äºå…¶ä»–æ¨¡å‹
2. **Recall vs Precisionæƒè¡¡**: 
   - XGBoost: æœ€é«˜recall (68.46%)ï¼Œé€‚åˆæ•è·æ›´å¤šå†å…¥é™¢æ‚£è€…
   - Random Forest: æ›´å¹³è¡¡çš„precision (39.15%)
3. **ç‰¹å¾é€‰æ‹©æ•ˆæœæ˜¾è‘—**: ä½¿ç”¨ä»…18ä¸ªç‰¹å¾å°±è¾¾åˆ°äº†0.70+çš„AUC

---

## ğŸ“ˆ è¯¦ç»†æŒ‡æ ‡

### Logistic Regression
- ROC-AUC: {df[df['model']=='LR']['roc_auc'].values[0]:.4f}
- ä¼˜åŠ¿: è®­ç»ƒå¿«é€Ÿï¼Œå¯è§£é‡Šæ€§å¼º
- é€‚ç”¨åœºæ™¯: éœ€è¦å¿«é€Ÿéƒ¨ç½²å’Œè§£é‡Šçš„åœºæ™¯

### Random Forest  
- ROC-AUC: {df[df['model']=='RF']['roc_auc'].values[0]:.4f}
- ä¼˜åŠ¿: è‡ªåŠ¨å¤„ç†éçº¿æ€§å…³ç³»ï¼Œç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
- é€‚ç”¨åœºæ™¯: éœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æ

### XGBoost â­
- ROC-AUC: {df[df['model']=='XGB']['roc_auc'].values[0]:.4f}
- ä¼˜åŠ¿: æœ€ä½³æ€§èƒ½ï¼Œå¤„ç†å¤æ‚æ¨¡å¼
- é€‚ç”¨åœºæ™¯: ç”Ÿäº§ç¯å¢ƒé¦–é€‰

---

## ğŸ“ æ–‡ä»¶ä½ç½®

- æ¨¡å‹: `artifacts/*.pkl`
- é¢„æµ‹ç»“æœ: `reports/predictions_*.csv`
- å¯è§†åŒ–: `reports/*.png`
- è¯¦ç»†æŒ‡æ ‡: `reports/metrics.csv`

---

## ğŸ”§ ä¸‹ä¸€æ­¥å»ºè®®

1. **è¶…å‚æ•°è°ƒä¼˜**: ä½¿ç”¨GridSearchä¼˜åŒ–XGBoost
2. **ç‰¹å¾å·¥ç¨‹**: å°è¯•å¢åŠ æ›´å¤šLASSOç‰¹å¾ (top_n: 100)
3. **é›†æˆå­¦ä¹ **: ç»„åˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹
4. **æ·±åº¦å­¦ä¹ **: è®­ç»ƒLSTMå’ŒTransformeræ¨¡å‹
5. **æ¨¡å‹è§£é‡Š**: ä½¿ç”¨SHAPåˆ†æç‰¹å¾é‡è¦æ€§
"""

with open('reports/MODEL_COMPARISON_REPORT.md', 'w') as f:
    f.write(md_report)

print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜: reports/MODEL_COMPARISON_REPORT.md")

print("\n" + "="*80)
print("âœ… æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
print("="*80)
