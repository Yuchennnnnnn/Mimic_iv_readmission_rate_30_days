# MIMIC-IV 30-Day Readmission Prediction Pipeline# 30-Day Hospital Readmission Prediction Pipeline# Yuchen Zhou's ML Pipeline for 30-Day Readmission Prediction



å®Œæ•´çš„æ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒPipelineï¼Œç”¨äºé¢„æµ‹MIMIC-IVæ•°æ®é›†ä¸­çš„30å¤©å†å…¥é™¢ç‡ã€‚



**ä½œè€…**: Yuchen Zhou, Jiaqi  **Author**: Yuchen Zhou  **Author:** Yuchen Zhou  

**è¯¾ç¨‹**: CS526 Machine Learning  

**æœºæ„**: Duke University**Course**: CompSci 526 - Fall 2025  **Date:** October 20, 2025  



---**Institution**: Duke University  **Course:** CS526 - Machine Learning in Healthcare, Duke University



## ğŸ“‹ ç›®å½•**Dataset**: MIMIC-IV Clinical Database  



- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)---

- [é¡¹ç›®ç»“æ„](#-é¡¹ç›®ç»“æ„)

- [å®‰è£…æŒ‡å—](#-å®‰è£…æŒ‡å—)---

- [æ•°æ®é¢„å¤„ç†](#-æ•°æ®é¢„å¤„ç†)

- [æ•°æ®æ ¼å¼](#-æ•°æ®æ ¼å¼)## ğŸ“ What's in This Folder

- [ä½¿ç”¨ç¤ºä¾‹](#-ä½¿ç”¨ç¤ºä¾‹)

- [ä¸Šä¼ GitHubæŒ‡å—](#-ä¸Šä¼ githubæŒ‡å—)## ğŸ“‹ Project Overview

- [æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)

- [æ•°æ®ç»Ÿè®¡](#-æ•°æ®ç»Ÿè®¡)This is my individual contribution to the team project. It contains a complete, independent ML pipeline that works alongside my teammates' work.



---This pipeline predicts **30-day hospital readmission risk** using MIMIC-IV clinical data. It implements **5 machine learning models** with automated feature selection based on LASSO coefficients from collaborative work.



## ğŸš€ å¿«é€Ÿå¼€å§‹```



### æœ€ç®€å•çš„è¿è¡Œæ–¹å¼### Key FeaturesYuchenZhou_Pipeline/



```bash- âœ… **Automated Feature Selection**: Uses pre-computed LASSO feature importanceâ”œâ”€â”€ training/              # My training pipeline

# 1. å…‹éš†ä»“åº“

git clone https://github.com/Yuchennnnnnn/Mimic_iv_readmission_rate_30_days.git- âœ… **5 Model Implementations**: LR, RF, XGBoost, LSTM, Transformerâ”œâ”€â”€ testing/               # My inference/testing pipeline

cd Mimic_iv_readmission_rate_30_days/YuchenZhou_jiaqi_Pipeline

- âœ… **Comprehensive Evaluation**: ROC-AUC, PR-AUC, Confusion Matrix, Calibrationâ”œâ”€â”€ README.md              # This file

# 2. å®‰è£…ä¾èµ–

cd preprocessing- âœ… **Production-Ready**: Modular code with config-driven trainingâ”œâ”€â”€ QUICK_REFERENCE.md     # Quick command reference

pip install -r requirements.txt

- âœ… **Complete Pipeline**: From data loading to model deploymentâ”œâ”€â”€ PIPELINE_README.md     # Detailed documentation

# 3. é…ç½®æ•°æ®è·¯å¾„

# ç¼–è¾‘ config.yamlï¼Œè®¾ç½®ä½ çš„MIMIC-IVæ•°æ®ç›®å½•â””â”€â”€ setup_and_run.sh       # Quick setup script



# 4. è¿è¡Œé¢„å¤„ç†ï¼ˆçº¦3-4å°æ—¶ï¼‰---```

bash run_all.sh



# 5. ä½¿ç”¨è¾“å‡ºæ•°æ®è®­ç»ƒæ¨¡å‹

python your_training_script.py## ğŸ† Model Performance Summary---

```



---

**Dataset**: 205,980 hospital admissions (26.72% readmission rate)  ## ğŸš€ Quick Start (2 Minutes)

## ğŸ“ é¡¹ç›®ç»“æ„

**Features**: 18 selected features (from 48 LASSO features)  

```

YuchenZhou_jiaqi_Pipeline/**Train/Test Split**: 80/20 (164,784 / 41,196 samples)  ### Option 1: Automated Quick Test

â”œâ”€â”€ preprocessing/                    # æ•°æ®é¢„å¤„ç†æ¨¡å—

â”‚   â”œâ”€â”€ scripts/                     # é¢„å¤„ç†è„šæœ¬```bash

â”‚   â”‚   â”œâ”€â”€ step1_load_data_optimized.py   # Step 1: åŠ è½½æ•°æ®

â”‚   â”‚   â”œâ”€â”€ step2_clean_units.py           # Step 2: æ¸…ç†å•ä½### Best Models by Metriccd YuchenZhou_Pipeline/training

â”‚   â”‚   â”œâ”€â”€ step3_create_timeseries.py     # Step 3: åˆ›å»ºæ—¶é—´åºåˆ—

â”‚   â”‚   â”œâ”€â”€ step4_compute_features.py      # Step 4: è®¡ç®—ç‰¹å¾python quick_start.py

â”‚   â”‚   â”œâ”€â”€ step5_temporal_split.py        # Step 5: æ—¶é—´åˆ†å‰²

â”‚   â”‚   â”œâ”€â”€ step6_save_output.py           # Step 6: ä¿å­˜è¾“å‡º| Metric | Model | Score | Notes |```

â”‚   â”‚   â””â”€â”€ utils.py                       # å·¥å…·å‡½æ•°

â”‚   â”œâ”€â”€ config.yaml                  # é…ç½®æ–‡ä»¶ âš™ï¸|--------|-------|-------|-------|

â”‚   â”œâ”€â”€ requirements.txt             # Pythonä¾èµ–

â”‚   â”œâ”€â”€ run_all.sh                   # è¿è¡Œå®Œæ•´pipeline| **ROC-AUC** | **Transformer** | **0.7056** â­ | Best overall discrimination |This will:

â”‚   â”œâ”€â”€ run_steps_2_to_6.sh         # è¿è¡Œæ­¥éª¤2-6

â”‚   â”œâ”€â”€ check_progress.sh            # æ£€æŸ¥è¿›åº¦| **F1-Score** | **XGBoost** | **0.4947** â­ | Best precision-recall balance |1. Generate synthetic test data

â”‚   â”œâ”€â”€ QUICKSTART.md                # å¿«é€Ÿå¼€å§‹æŒ‡å—

â”‚   â””â”€â”€ README.md                    # é¢„å¤„ç†è¯¦ç»†æ–‡æ¡£| **Recall** | **XGBoost** | **68.5%** â­ | Catches most readmissions |2. Train a Logistic Regression model

â”œâ”€â”€ training/                         # æ¨¡å‹è®­ç»ƒæ¨¡å—

â”‚   â”œâ”€â”€ src/                         # è®­ç»ƒä»£ç | **Precision** | **Transformer** | **65.2%** | Lowest false positives |3. Evaluate and create reports

â”‚   â”œâ”€â”€ config.yaml                  # è®­ç»ƒé…ç½®

â”‚   â”œâ”€â”€ run_training.py              # è®­ç»ƒè„šæœ¬4. Show you where results are saved

â”‚   â””â”€â”€ requirements.txt             # è®­ç»ƒä¾èµ–

â”œâ”€â”€ testing/                          # æ¨¡å‹æµ‹è¯•æ¨¡å—### Complete Results

â”‚   â”œâ”€â”€ src/                         # æµ‹è¯•ä»£ç 

â”‚   â””â”€â”€ run_inference.sh             # æ¨ç†è„šæœ¬### Option 2: Use Bash Script

â”œâ”€â”€ output/                           # è¾“å‡ºæ•°æ® (39GB, ä¸ä¸Šä¼ )

â”‚   â”œâ”€â”€ train_data.pkl               # è®­ç»ƒé›† (10GB)| Model | ROC-AUC | PR-AUC | Accuracy | Precision | Recall | F1-Score |```bash

â”‚   â”œâ”€â”€ val_data.pkl                 # éªŒè¯é›† (2.9GB)

â”‚   â”œâ”€â”€ test_data.pkl                # æµ‹è¯•é›† (2.4GB)|-------|---------|--------|----------|-----------|--------|----------|cd YuchenZhou_Pipeline

â”‚   â”œâ”€â”€ train_index.parquet          # è®­ç»ƒé›†ç´¢å¼• (2.6MB)

â”‚   â”œâ”€â”€ val_index.parquet            # éªŒè¯é›†ç´¢å¼• (781KB)| **Transformer** | **0.7056** | 0.4778 | 74.84% | 65.20% | 12.55% | 0.2104 |chmod +x setup_and_run.sh

â”‚   â”œâ”€â”€ test_index.parquet           # æµ‹è¯•é›†ç´¢å¼• (638KB)

â”‚   â””â”€â”€ feature_names.txt            # ç‰¹å¾åç§°| **XGBoost** | **0.7040** | **0.4756** | 62.60% | 38.72% | **68.52%** | **0.4947** |./setup_and_run.sh

â”œâ”€â”€ .gitignore                        # Gitå¿½ç•¥æ–‡ä»¶é…ç½®

â””â”€â”€ README.md                         # æœ¬æ–‡ä»¶| **LSTM** | 0.7030 | 0.4723 | 74.83% | 62.56% | 14.50% | 0.2354 |```

```

| **Random Forest** | 0.6941 | 0.4625 | 64.25% | 39.37% | 62.59% | 0.4834 |

---

| **Logistic Reg** | 0.6626 | 0.4037 | 59.18% | 35.76% | 66.21% | 0.4643 |---

## ğŸ”§ å®‰è£…æŒ‡å—



### ç³»ç»Ÿè¦æ±‚

### Key Insights## ğŸ“Š Train on Real Data

- **æ“ä½œç³»ç»Ÿ**: macOS / Linux / Windows (WSL)

- **Python**: 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬

- **å†…å­˜**: è‡³å°‘16GB RAMï¼ˆæ¨è32GBï¼‰

- **å­˜å‚¨**: è‡³å°‘40GBå¯ç”¨ç£ç›˜ç©ºé—´1. **XGBoost is the best practical choice**:### Step 1: Install Dependencies

- **MIMIC-IVè®¿é—®**: éœ€è¦é€šè¿‡[PhysioNet](https://physionet.org/)è·å¾—è®¿é—®æƒé™

   - High recall (68.5%) catches most readmissions```bash

### æ­¥éª¤1: å®‰è£…Pythonä¾èµ–

   - Balanced F1-score (0.49) for real-world deploymentcd YuchenZhou_Pipeline/training

```bash

cd YuchenZhou_jiaqi_Pipeline/preprocessing   - Fastest training among top performerspip install -r requirements.txt

pip install -r requirements.txt

``````



**å…³é”®ä¾èµ–è¯´æ˜**:2. **Transformer has highest AUC but low recall**:



```txt   - Best at ranking risk (0.7056 AUC)### Step 2: Update Configuration

pandas==2.0.3          # âš ï¸ å¿…é¡»æ˜¯2.0.3ç‰ˆæœ¬ï¼ˆå…¼å®¹æ€§ï¼‰

numpy>=1.26.4          # æ•°å€¼è®¡ç®—   - Very high precision (65%) but misses many cases (12% recall)Edit `training/config.yaml` to point to the cleaned data:

fastparquet==2024.11.0 # Parquetæ–‡ä»¶è¯»å†™

PyYAML                 # é…ç½®æ–‡ä»¶è§£æ   - Better suited for high-confidence predictions```yaml

tqdm                   # è¿›åº¦æ¡

scikit-learn           # æ•°æ®å¤„ç†data:

torch                  # æ¨¡å‹è®­ç»ƒï¼ˆå¯é€‰ï¼‰

```3. **Traditional ML vs Deep Learning**:  input_path: "../../cleaned_data.csv"  # Adjust path as needed



### æ­¥éª¤2: é…ç½®æ•°æ®è·¯å¾„   - XGBoost/RF: Better recall, faster training, easier deployment```



ç¼–è¾‘ `preprocessing/config.yaml`:   - LSTM/Transformer: Higher precision, better calibration, requires more compute



```yaml### Step 3: Train Models

data_paths:

  patients: "/path/to/mimic-iv-3.1/hosp/patients.csv"---```bash

  admissions: "/path/to/mimic-iv-3.1/hosp/admissions.csv"

  chartevents: "/path/to/mimic-iv-3.1/icu/chartevents.csv"      # 39GB# Train all 5 models

  labevents: "/path/to/mimic-iv-3.1/hosp/labevents.csv"         # 17GB

  prescriptions: "/path/to/mimic-iv-3.1/hosp/prescriptions.csv" # å¯é€‰## ğŸ“ Project Structurepython src/train.py --model all --config config.yaml



paths:

  output_dir: "../output"  # è¾“å‡ºç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰

```# Or train individually

preprocessing:

  time_window_hours: 48     # æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰YuchenZhou_Pipeline/python src/train.py --model logistic --config config.yaml

  bin_size_hours: 1         # æ—¶é—´åˆ†è¾¨ç‡ï¼ˆå°æ—¶ï¼‰

  min_age: 18               # æœ€å°å¹´é¾„â”œâ”€â”€ README.md                              # This filepython src/train.py --model rf --config config.yaml

  readmit_window_days: 30   # å†å…¥é™¢çª—å£ï¼ˆå¤©ï¼‰

  chunk_size: 10000         # å¤„ç†å—å¤§å°â”œâ”€â”€ FEATURE_SELECTION_EXPLANATION.md       # Why 50 features â†’ 18 columnspython src/train.py --model xgb --config config.yaml

```

â”œâ”€â”€ Feature_Importance_by_Coef.csv         # LASSO coefficients (Xi Chen's work)python src/train.py --model lstm --config config.yaml --epochs 30

---

â”‚python src/train.py --model transformer --config config.yaml

## ğŸ”„ æ•°æ®é¢„å¤„ç†

â”œâ”€â”€ training/```

### Pipelineå·¥ä½œæµç¨‹

â”‚   â”œâ”€â”€ config.yaml                        # Configuration file

```

MIMIC-IVåŸå§‹æ•°æ®â”‚   â”œâ”€â”€ requirements.txt                   # Python dependencies---

    â†“

Step 1: åŠ è½½å’Œè¿‡æ»¤æ•°æ® (30-40åˆ†é’Ÿ)â”‚   â”œâ”€â”€ quick_train.sh                     # One-click training script

    â”œâ”€ ç­›é€‰å¹´é¾„â‰¥18å²

    â”œâ”€ ç­›é€‰ä½é™¢æ—¶é•¿â‰¥48å°æ—¶â”‚   â”œâ”€â”€ run_training.py                    # Interactive training## ğŸ“ˆ View Results

    â”œâ”€ æ’é™¤é™¢å†…æ­»äº¡

    â””â”€ è®¡ç®—30å¤©å†å…¥é™¢æ ‡ç­¾â”‚   â”œâ”€â”€ check_feature_mapping.py           # Feature analysis tool

    â†“

Step 2: æ¸…ç†å’Œæ ‡å‡†åŒ–å•ä½ (5-10åˆ†é’Ÿ)â”‚   â”‚### Check Metrics

    â”œâ”€ æ¸©åº¦: Fahrenheit â†’ Celsius

    â”œâ”€ æ˜ å°„itemidåˆ°ç‰¹å¾åâ”‚   â”œâ”€â”€ src/```bash

    â””â”€ å»é‡

    â†“â”‚   â”‚   â”œâ”€â”€ train.py                       # Main training scriptcd training

Step 3: åˆ›å»º48å°æ—¶æ—¶é—´åºåˆ— (2-3å°æ—¶) â°

    â”œâ”€ å°†äº‹ä»¶åˆ†åˆ°48ä¸ª1å°æ—¶binsâ”‚   â”‚   â”œâ”€â”€ feature_selection.py           # Feature selection logic â­cat reports/metrics.csv

    â”œâ”€ èšåˆæ¯ä¸ªbinçš„è§‚æµ‹å€¼

    â””â”€ å¤„ç†322,966ä¸ªä½é™¢è®°å½•â”‚   â”‚   â”œâ”€â”€ preprocess.py                  # Data preprocessing```

    â†“

Step 4: è®¡ç®—Maskså’ŒDeltasç‰¹å¾ (5-10åˆ†é’Ÿ)â”‚   â”‚   â”œâ”€â”€ models.py                      # Model implementations

    â”œâ”€ Masks: è§‚æµ‹æŒ‡ç¤ºå™¨ (1=æœ‰è§‚æµ‹, 0=ç¼ºå¤±)

    â”œâ”€ Deltas: è·ç¦»ä¸Šæ¬¡è§‚æµ‹çš„æ—¶é—´å·®â”‚   â”‚   â”œâ”€â”€ evaluate.py                    # Evaluation metrics### View Plots

    â””â”€ Forward-fillå¡«å……

    â†“â”‚   â”‚   â”œâ”€â”€ dataset.py                     # PyTorch datasets```bash

Step 5: æ—¶é—´åˆ†å‰² (2-3åˆ†é’Ÿ)

    â”œâ”€ è®­ç»ƒé›†: 2008-2013 (60%)â”‚   â”‚   â””â”€â”€ utils.py                       # Helper functions# ROC curves

    â”œâ”€ éªŒè¯é›†: 2014-2016 (20%)

    â””â”€ æµ‹è¯•é›†: 2017-2019 (20%)â”‚   â”‚open reports/roc_curve_xgb.png

    â†“

Step 6: ä¿å­˜æœ€ç»ˆè¾“å‡º (1-2åˆ†é’Ÿ)â”‚   â”œâ”€â”€ artifacts/                         # Trained models

    â”œâ”€ train_data.pkl (10GB)

    â”œâ”€ val_data.pkl (2.9GB)â”‚   â”‚   â”œâ”€â”€ lr.pkl                         # Logistic Regression# Model comparison

    â””â”€ test_data.pkl (2.4GB)

    â†“â”‚   â”‚   â”œâ”€â”€ rf.pkl                         # Random Forestopen reports/model_comparison.png

æœ€ç»ˆè®­ç»ƒæ•°æ® (39GB total)

```â”‚   â”‚   â”œâ”€â”€ xgb.pkl                        # XGBoost â­



### è¿è¡Œæ–¹å¼â”‚   â”‚   â”œâ”€â”€ lstm.pth                       # LSTM# Feature importance



#### æ–¹å¼1: ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰â”‚   â”‚   â””â”€â”€ transformer.pth                # Transformeropen reports/feature_importance_xgb.png



```bashâ”‚   â”‚```

cd preprocessing

bash run_all.shâ”‚   â””â”€â”€ reports/                           # Evaluation results

```

â”‚       â”œâ”€â”€ metrics.csv                    # All model metrics---

#### æ–¹å¼2: åˆ†æ­¥è¿è¡Œ

â”‚       â”œâ”€â”€ predictions_*.csv              # Model predictions

```bash

cd preprocessingâ”‚       â”œâ”€â”€ roc_curve_*.png                # ROC curves## ğŸ§ª Make Predictions on New Data



# Step 1: åŠ è½½æ•°æ®â”‚       â”œâ”€â”€ confusion_matrix_*.png         # Confusion matrices

python scripts/step1_load_data_optimized.py

â”‚       â””â”€â”€ feature_importance_*.png       # Feature importance plots```bash

# Steps 2-6: å¤„ç†å’Œä¿å­˜

bash run_steps_2_to_6.shâ”‚cd YuchenZhou_Pipeline/testing

```

â””â”€â”€ testing/

#### æ–¹å¼3: åå°è¿è¡Œï¼ˆæ¨èç”¨äºé•¿æ—¶é—´ä»»åŠ¡ï¼‰

    â”œâ”€â”€ src/python src/inference.py \

```bash

cd preprocessing    â”‚   â””â”€â”€ inference.py                   # Model inference script  --model-path ../training/artifacts/xgb.pkl \



# åœ¨åå°è¿è¡Œæ•´ä¸ªpipeline    â””â”€â”€ README.md                          # Testing documentation  --preprocessor-path ../training/artifacts/xgb_preprocessor.joblib \

nohup bash run_all.sh > full_pipeline.log 2>&1 &

```  --input ../../cleaned_data.csv \

# æŸ¥çœ‹å®æ—¶æ—¥å¿—

tail -f full_pipeline.log  --output my_predictions.csv \



# æ£€æŸ¥è¿›åº¦---  --model-type sklearn

./check_progress.sh

``````



#### æ–¹å¼4: å•ç‹¬è¿è¡ŒæŸä¸ªæ­¥éª¤## ğŸš€ Quick Start



```bash---

cd preprocessing

### 1. Environment Setup

# è¿è¡Œç‰¹å®šæ­¥éª¤

python scripts/step3_create_timeseries.py## ğŸ“š Documentation

python scripts/step4_compute_features.py

# ... ç­‰ç­‰```bash

```

# Navigate to project- **QUICK_REFERENCE.md** - Command cheat sheet

### ç›‘æ§è¿›åº¦

cd YuchenZhou_Pipeline/training- **PIPELINE_README.md** - Complete documentation

```bash

# æ–¹æ³•1: ä½¿ç”¨ç›‘æ§è„šæœ¬- **IMPLEMENTATION_SUMMARY.md** - What was built

cd preprocessing

./check_progress.sh# Install dependencies (using virtual environment)- **training/README.md** - Training details



# æ–¹æ³•2: æŸ¥çœ‹æ—¥å¿—pip install -r requirements.txt- **testing/README.md** - Inference details

tail -f full_pipeline.log

```

# æ–¹æ³•3: æ£€æŸ¥è¾“å‡ºæ–‡ä»¶

ls -lh output/---



# æ–¹æ³•4: æŸ¥çœ‹è¿è¡Œè¿›ç¨‹**Required Packages**:

ps aux | grep python | grep step

```- numpy<2, pandas, scikit-learn## ğŸ¯ Models Implemented



**é¢„æœŸè¿è¡Œæ—¶é—´**:- xgboost, imbalanced-learn

- Step 1: 30-40åˆ†é’Ÿ

- Step 2: 5-10åˆ†é’Ÿ- torch, torchvision1. **Logistic Regression** - Interpretable baseline

- Step 3: 2-3å°æ—¶ â°ï¼ˆæœ€æ…¢ï¼‰

- Step 4: 5-10åˆ†é’Ÿ- matplotlib, seaborn, pyyaml2. **Random Forest** - Ensemble with feature importance

- Step 5: 2-3åˆ†é’Ÿ

- Step 6: 1-2åˆ†é’Ÿ3. **XGBoost** - Best performance (typically)

- **æ€»è®¡**: çº¦3-4å°æ—¶

### 2. Train Models4. **LSTM** - Deep learning with embeddings

---

5. **Transformer** - Attention-based architecture

## ğŸ“Š æ•°æ®æ ¼å¼

**Option A: Interactive Script (Recommended)**

### Pickleæ–‡ä»¶ç»“æ„

```bash---

```python

# åŠ è½½æ•°æ®ç¤ºä¾‹./quick_train.sh

import pickle

## ğŸ“‚ Expected Outputs

with open('output/train_data.pkl', 'rb') as f:

    data_dict = pickle.load(f)# Then select:



# æ•°æ®ç»“æ„# 1 - Quick test (Logistic Regression, ~2 min)After training, you'll have:

{

    'data': [  # æ ·æœ¬åˆ—è¡¨# 2 - Traditional ML (LR + RF + XGBoost, ~15 min) â­

        {

            'hadm_id': 12345678,           # ä½é™¢ID# 3 - All models (including LSTM + Transformer, ~1 hour)```

            'subject_id': 10000001,        # æ‚£è€…ID

            'admittime': datetime(...),    # å…¥é™¢æ—¶é—´```training/

            'values': np.array(...),       # (48, 49) æ—¶é—´åºåˆ—æ•°å€¼

            'masks': np.array(...),        # (48, 49) è§‚æµ‹æŒ‡ç¤ºå™¨â”œâ”€â”€ artifacts/

            'deltas': np.array(...),       # (48, 49) æ—¶é—´å·®

            'readmit_30d': 0,              # 0=æ— å†å…¥é™¢, 1=æœ‰å†å…¥é™¢**Option B: Manual Training**â”‚   â”œâ”€â”€ lr.pkl, rf.pkl, xgb.pkl        # Trained models

            'anchor_year_group': '2008 - 2010'

        },```bashâ”‚   â”œâ”€â”€ lstm.pt, transformer.pt        # PyTorch models

        # ... 194,672ä¸ªè®­ç»ƒæ ·æœ¬

    ],# Single modelâ”‚   â””â”€â”€ *_preprocessor.joblib          # Encoders

    'feature_names': [  # 49ä¸ªç‰¹å¾åç§°

        'heart_rate', 'sbp', 'dbp', 'temperature', python src/train.py --model xgb --config config.yamlâ””â”€â”€ reports/

        'respiratory_rate', 'spo2', 'glucose', ...

    ]    â”œâ”€â”€ metrics.csv                     # All metrics

}

```# Multiple models    â”œâ”€â”€ model_comparison.png            # Comparison chart



### æ—¶é—´åºåˆ—ç»´åº¦è¯´æ˜python src/train.py --model logistic --config config.yaml    â”œâ”€â”€ roc_curve_*.png                 # ROC curves



- **Shape**: `(48, 49)`python src/train.py --model rf --config config.yaml    â”œâ”€â”€ predictions_*.csv               # Predictions

  - **48**: æ—¶é—´æ­¥ï¼ˆå…¥é™¢åå‰48å°æ—¶ï¼Œæ¯å°æ—¶1æ­¥ï¼‰

  - **49**: ç‰¹å¾æ•°é‡python src/train.py --model xgb --config config.yaml    â””â”€â”€ feature_importance_*.png        # Feature importance



- **ç‰¹å¾ç»„æˆ**:``````

  - **19ä¸ªç”Ÿå‘½ä½“å¾**: å¿ƒç‡ã€æ”¶ç¼©å‹ã€èˆ’å¼ å‹ã€ä½“æ¸©ã€å‘¼å¸é¢‘ç‡ã€è¡€æ°§ç­‰

  - **30ä¸ªå®éªŒå®¤æŒ‡æ ‡**: è¡€ç³–ã€ç™½ç»†èƒã€è‚Œé…ã€è¡€çº¢è›‹ç™½ã€é’ ã€é’¾ç­‰



- **ä¸‰ä¸ªæ•°ç»„**:### 3. View Results---

  - **values**: å®é™…è§‚æµ‹å€¼

  - **masks**: 1=è¯¥æ—¶é—´ç‚¹æœ‰è§‚æµ‹, 0=ç¼ºå¤±

  - **deltas**: è·ç¦»ä¸Šæ¬¡è§‚æµ‹ç»è¿‡çš„å°æ—¶æ•°

```bash## âš¡ Performance Summary

### Parquetç´¢å¼•æ–‡ä»¶

cd reports/

```python

import pandas as pdExpected results on MIMIC-IV data:



# åŠ è½½ç´¢å¼•# View metrics

train_index = pd.read_parquet('output/train_index.parquet', engine='fastparquet')

cat metrics.csv| Model | ROC-AUC | Training Time |

# ç´¢å¼•åŒ…å«:

# - file_idx: åœ¨pickleæ–‡ä»¶ä¸­çš„ç´¢å¼•|-------|---------|---------------|

# - hadm_id: ä½é™¢ID

# - subject_id: æ‚£è€…ID# View visualizations| Logistic Reg | 0.67 | 1 min |

# - admittime: å…¥é™¢æ—¶é—´

# - readmit_30d: å†å…¥é™¢æ ‡ç­¾open roc_curve_xgb.png| Random Forest | 0.71 | 5 min |

# - anchor_year_group: å¹´ä»½ç»„

```open confusion_matrix_xgb.png| **XGBoost** | **0.73** | 5 min â­ |



---open feature_importance_xgb.png| LSTM | 0.71 | 30 min |



## ğŸ’» ä½¿ç”¨ç¤ºä¾‹```| Transformer | 0.72 | 40 min |



### ç¤ºä¾‹1: åŠ è½½å’Œæ¢ç´¢æ•°æ®



```python------

import pickle

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt## ğŸ”§ Configuration## ğŸ”§ Troubleshooting



# 1. åŠ è½½è®­ç»ƒæ•°æ®

with open('output/train_data.pkl', 'rb') as f:

    train_dict = pickle.load(f)Edit `training/config.yaml` to customize:### GPU Out of Memory



train_data = train_dict['data']```yaml

feature_names = train_dict['feature_names']

### Feature Selection# In training/config.yaml, reduce batch size:

print(f"âœ“ Loaded {len(train_data)} training samples")

print(f"âœ“ Features: {len(feature_names)}")```yamlhyperparameters:

print(f"âœ“ First 10 features: {feature_names[:10]}")

feature_selection:  lstm:

# 2. æŸ¥çœ‹å•ä¸ªæ ·æœ¬

sample = train_data[0]  enabled: true                    # Use LASSO feature selection    batch_size: 32

print(f"\n=== Sample Structure ===")

print(f"Admission ID: {sample['hadm_id']}")  top_n: 50                        # Number of top features (current: 18 columns)```

print(f"Patient ID: {sample['subject_id']}")

print(f"Admission time: {sample['admittime']}")  importance_threshold: 0.05       # Minimum importance threshold

print(f"Readmission label: {sample['readmit_30d']}")

print(f"\nData shapes:")  feature_importance_path: "../Feature_Importance_by_Coef.csv"### Import Errors

print(f"  values: {sample['values'].shape}")    # (48, 49)

print(f"  masks:  {sample['masks'].shape}")     # (48, 49)```Make sure you're in the correct directory:

print(f"  deltas: {sample['deltas'].shape}")    # (48, 49)

```bash

# 3. å¯è§†åŒ–ä¸€ä¸ªç‰¹å¾çš„æ—¶é—´åºåˆ—

feature_idx = 0  # heart_rate**Note**: 50 LASSO features map to 18 original columns due to one-hot encoding. See [FEATURE_SELECTION_EXPLANATION.md](./FEATURE_SELECTION_EXPLANATION.md) for details.cd YuchenZhou_Pipeline/training

time_series = sample['values'][:, feature_idx]

mask = sample['masks'][:, feature_idx]python src/train.py --model logistic



plt.figure(figsize=(12, 4))### Model Hyperparameters```

plt.plot(time_series, marker='o', label='Heart Rate')

plt.scatter(np.where(mask == 0)[0], ```yaml

           time_series[mask == 0], 

           color='red', s=100, label='Missing')models:### Slow Training

plt.xlabel('Hour')

plt.ylabel('Heart Rate')  logistic:Train on a subset first to test:

plt.title(f'Heart Rate over 48 hours (Admission {sample["hadm_id"]})')

plt.legend()    penalty: 'l2'```python

plt.grid(True)

plt.show()    C: 1.0# In config.yaml, you can add this in preprocessing:



# 4. ç»Ÿè®¡ä¿¡æ¯    max_iter: 1000preprocessing:

readmit_count = sum([s['readmit_30d'] for s in train_data])

print(f"\n=== Dataset Statistics ===")    class_weight: 'balanced'  sample_size: 5000  # Use subset for testing

print(f"Total samples: {len(train_data)}")

print(f"Readmissions: {readmit_count} ({readmit_count/len(train_data)*100:.2f}%)")  ```

print(f"Non-readmissions: {len(train_data)-readmit_count}")

```  rf:



### ç¤ºä¾‹2: PyTorchæ•°æ®åŠ è½½å™¨    n_estimators: 100---



```python    max_depth: 15

import torch

from torch.utils.data import Dataset, DataLoader    min_samples_split: 50## ğŸ¤ Relation to Team Project

import numpy as np

    class_weight: 'balanced'

class MIMICDataset(Dataset):

    """MIMIC-IV 30-day readmission dataset"""  This pipeline is my **independent contribution** and works separately from:

    

    def __init__(self, data_list):  xgb:- `preprocessing/` - Shared preprocessing scripts

        self.data = data_list

        n_estimators: 200- `XiChen_Lasso/` - Xi Chen's LASSO feature selection

    def __len__(self):

        return len(self.data)    max_depth: 5- Other teammates' work

    

    def __getitem__(self, idx):    learning_rate: 0.1

        sample = self.data[idx]

            scale_pos_weight: 3.0All folders can coexist and use the same `cleaned_data.csv` file.

        return {

            'values': torch.FloatTensor(sample['values']),      # (48, 49)```

            'masks': torch.FloatTensor(sample['masks']),        # (48, 49)

            'deltas': torch.FloatTensor(sample['deltas']),      # (48, 49)---

            'label': torch.LongTensor([sample['readmit_30d']]), # (1,)

            'hadm_id': sample['hadm_id']### Training Parameters

        }

```yaml## ğŸ“ What I Contributed

# åŠ è½½æ•°æ®

import picklesplit:

with open('output/train_data.pkl', 'rb') as f:

    train_dict = pickle.load(f)  test_size: 0.2âœ… Complete end-to-end ML pipeline  

with open('output/val_data.pkl', 'rb') as f:

    val_dict = pickle.load(f)  random_state: 42âœ… 5 different model implementations  



# åˆ›å»ºæ•°æ®é›†  stratify: trueâœ… Comprehensive preprocessing  

train_dataset = MIMICDataset(train_dict['data'])

val_dataset = MIMICDataset(val_dict['data'])âœ… Full evaluation suite  



# åˆ›å»ºæ•°æ®åŠ è½½å™¨deep_learning:âœ… Deployment infrastructure  

train_loader = DataLoader(

    train_dataset,  epochs: 50âœ… 3,500+ lines of code  

    batch_size=64,

    shuffle=True,  batch_size: 256âœ… 2,000+ lines of documentation  

    num_workers=4,

    pin_memory=True  learning_rate: 0.001âœ… Unit and integration tests  

)

  early_stopping_patience: 5

val_loader = DataLoader(

    val_dataset,```---

    batch_size=128,

    shuffle=False,

    num_workers=4

)---## ğŸ“ For Grading/Presentation



# ä½¿ç”¨ç¤ºä¾‹

for batch in train_loader:

    values = batch['values']     # (batch_size, 48, 49)## ğŸ“Š Selected Features (18 Total)**Key Files to Review:**

    masks = batch['masks']       # (batch_size, 48, 49)

    deltas = batch['deltas']     # (batch_size, 48, 49)1. `training/src/train.py` - Main training script

    labels = batch['label']      # (batch_size, 1)

    ### Demographic (3 features)2. `training/src/models.py` - All 5 models

    print(f"Batch shapes:")

    print(f"  values: {values.shape}")- `anchor_age` - Patient age3. `training/reports/metrics.csv` - Results

    print(f"  masks: {masks.shape}")

    print(f"  deltas: {deltas.shape}")- `gender` - Patient gender (F/M)4. `training/reports/model_comparison.png` - Visual comparison

    print(f"  labels: {labels.shape}")

    break- `marital_status` - Marital status

```

**To Demonstrate:**

### ç¤ºä¾‹3: LSTMæ¨¡å‹è®­ç»ƒ

### Clinical (4 features)```bash

```python

import torch- `died_in_hospital` - In-hospital mortality (â­ Most important, weight=0.60)# Quick test (2 min)

import torch.nn as nn

import torch.optim as optim- `days_since_prev_discharge` - Time since last dischargecd training

from sklearn.metrics import roc_auc_score, accuracy_score

- `num_diagnoses` - Number of diagnosespython quick_start.py

class LSTMReadmissionModel(nn.Module):

    """LSTMæ¨¡å‹ç”¨äº30å¤©å†å…¥é™¢é¢„æµ‹"""- `is_surgical_service` - Surgical vs medical service

    

    def __init__(self, input_dim=49, hidden_dim=128, num_layers=2, dropout=0.3):# View results

        super().__init__()

        ### Administrative (5 features)cat reports/metrics.csv

        # å¯ä»¥å°†values, masks, deltasæ‹¼æ¥ä½œä¸ºè¾“å…¥

        self.input_dim = input_dim * 3  # 49 * 3 = 147- `admission_type` - Type of admission (Emergency, Elective, etc.)```

        

        self.lstm = nn.LSTM(- `admission_location` - Where patient admitted from

            self.input_dim,

            hidden_dim,- `discharge_location` - Where patient discharged to---

            num_layers,

            batch_first=True,- `last_service` - Last clinical service (â­ 2nd most important)

            dropout=dropout if num_layers > 1 else 0

        )- `insurance` - Insurance type## ğŸ“§ Questions?

        

        self.dropout = nn.Dropout(dropout)

        self.fc = nn.Linear(hidden_dim, 2)  # Binary classification

    ### Lab Values (4 features)- Check `QUICK_REFERENCE.md` for commands

    def forward(self, values, masks, deltas):

        # values, masks, deltas: (batch, 48, 49)- `Glucose_median` - Median glucose level- See `PIPELINE_README.md` for full documentation

        

        # æ‹¼æ¥ä¸‰ä¸ªç‰¹å¾- `Hemoglobin_median` - Median hemoglobin- Review `training/README.md` for detailed guide

        x = torch.cat([values, masks, deltas], dim=-1)  # (batch, 48, 147)

        - `Hemoglobin_min` - Minimum hemoglobin

        # LSTM

        lstm_out, (h_n, c_n) = self.lstm(x)  # (batch, 48, hidden_dim)- `Potassium_min` - Minimum potassium---

        

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º

        last_output = lstm_out[:, -1, :]  # (batch, hidden_dim)

        ### Other (2 features)**Ready to use! Run `python training/quick_start.py` to get started.** ğŸš€

        # Dropout + å…¨è¿æ¥å±‚

        x = self.dropout(last_output)- `language` - Primary language

        logits = self.fc(x)  # (batch, 2)- `unique_careunits` - Number of care units visited

        

        return logits---



# åˆå§‹åŒ–æ¨¡å‹## ğŸ¯ Model Recommendations

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = LSTMReadmissionModel(### For Different Use Cases

    input_dim=49,

    hidden_dim=128,| Use Case | Recommended Model | Reason |

    num_layers=2,|----------|-------------------|--------|

    dropout=0.3| **Production Deployment** | **XGBoost** | Best F1-score, good recall, fast inference |

).to(device)| **High-Risk Screening** | **XGBoost** | 68.5% recall catches most readmissions |

| **Low False Alarm** | **Transformer** | 65% precision, lowest false positives |

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨| **Risk Ranking** | **Transformer** | Highest AUC (0.7056) for risk stratification |

criterion = nn.CrossEntropyLoss()| **Interpretability** | **Logistic Regression** | Clear coefficients, easy to explain |

optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)| **Quick Prototyping** | **Random Forest** | Fast training, good baseline performance |



# è®­ç»ƒå¾ªç¯### Deployment Strategy

num_epochs = 50

**Recommended Two-Stage Approach**:

for epoch in range(num_epochs):1. **Stage 1**: Use XGBoost for initial screening (high recall)

    model.train()2. **Stage 2**: Use Transformer to prioritize high-risk cases (high precision)

    train_loss = 0.0

    This catches most readmissions while minimizing unnecessary interventions.

    for batch in train_loader:

        values = batch['values'].to(device)---

        masks = batch['masks'].to(device)

        deltas = batch['deltas'].to(device)## ğŸ§ª Model Inference

        labels = batch['label'].squeeze().to(device)

        Use trained models for predictions:

        # å‰å‘ä¼ æ’­

        optimizer.zero_grad()```bash

        logits = model(values, masks, deltas)cd testing

        loss = criterion(logits, labels)

        # Using XGBoost (recommended)

        # åå‘ä¼ æ’­python src/inference.py \

        loss.backward()  --model ../training/artifacts/xgb.pkl \

        optimizer.step()  --data ../../cleaned_data.csv \

          --output predictions.csv

        train_loss += loss.item()

    # Output includes:

    # éªŒè¯# - Patient ID

    model.eval()# - True label

    val_preds = []# - Predicted probability

    val_labels = []# - Predicted class

    # - Risk category (Low/Medium/High)

    with torch.no_grad():```

        for batch in val_loader:

            values = batch['values'].to(device)---

            masks = batch['masks'].to(device)

            deltas = batch['deltas'].to(device)## ğŸ“ˆ Performance Analysis

            labels = batch['label'].squeeze()

            ### Confusion Matrix (XGBoost)

            logits = model(values, masks, deltas)```

            probs = torch.softmax(logits, dim=1)[:, 1]  # å–readmissionæ¦‚ç‡                Predicted

                          No      Yes

            val_preds.extend(probs.cpu().numpy())Actual  No   18,244  11,944   â†’ Specificity: 60.4%

            val_labels.extend(labels.numpy())        Yes   3,465   7,543   â†’ Sensitivity: 68.5%

    ```

    # è®¡ç®—æŒ‡æ ‡

    val_auc = roc_auc_score(val_labels, val_preds)**Interpretation**:

    val_acc = accuracy_score(val_labels, (np.array(val_preds) > 0.5).astype(int))- **True Positives (7,543)**: Correctly identified readmissions

    - **False Negatives (3,465)**: Missed readmissions (31.5%)

    print(f"Epoch {epoch+1}/{num_epochs}")- **False Positives (11,944)**: False alarms

    print(f"  Train Loss: {train_loss/len(train_loader):.4f}")- **True Negatives (18,244)**: Correct non-readmission predictions

    print(f"  Val AUC: {val_auc:.4f}")

    print(f"  Val Acc: {val_acc:.4f}")### ROC-AUC Comparison

```

All models achieve AUC > 0.66, with top 3 models > 0.70:

---- Excellent: 0.9-1.0

- Good: 0.8-0.9

## ğŸ“¤ ä¸Šä¼ GitHubæŒ‡å—- **Fair: 0.7-0.8** â† Our models

- Poor: 0.6-0.7

### âš ï¸ é‡è¦ï¼šGitHubå¤§æ–‡ä»¶é™åˆ¶- Random: 0.5



**GitHubé™åˆ¶**:Our models are in the **"Fair to Good"** range, suitable for clinical decision support.

- å•ä¸ªæ–‡ä»¶æœ€å¤§ **100MB**

- æ¨é€å¤§å°å»ºè®®å°äº **1GB**---

- ä»“åº“æ€»å¤§å°å»ºè®®å°äº **5GB**

## ğŸ”¬ Feature Importance (Top 10 from XGBoost)

**æˆ‘ä»¬çš„æ•°æ®è§„æ¨¡**:

- `output/` ç›®å½•: **39GB** âŒ å¤ªå¤§ï¼1. **died_in_hospital** (0.603) - In-hospital mortality

- å¤§æ–‡ä»¶åˆ—è¡¨:2. **last_service_OMED** (0.451) - Served by OMED

  ```3. **days_since_prev_discharge** (0.281) - Time since last visit

  17GB - timeseries_processed.pkl4. **gender** (0.382) - Patient gender

  10GB - train_data.pkl5. **discharge_location** (0.318) - Discharge destination

  5.7GB - timeseries_binned.pkl6. **admission_type** (0.340) - Type of admission

  2.9GB - val_data.pkl7. **anchor_age** (0.171) - Patient age

  2.4GB - test_data.pkl8. **insurance** (0.210) - Insurance type

  590MB - labevents_raw.parquet9. **marital_status** (0.208) - Marital status

  442MB - chartevents_raw.parquet10. **Hemoglobin_median** (0.107) - Median hemoglobin

  112MB - labevents_clean.parquet

  ```See `reports/feature_importance_xgb.png` for full visualization.



### æ–¹æ¡ˆ1: åªä¸Šä¼ ä»£ç ï¼ˆæ¨èï¼‰ âœ…---



**å·²é…ç½®`.gitignore`æ–‡ä»¶**ï¼Œè‡ªåŠ¨æ’é™¤å¤§æ–‡ä»¶ï¼š## ğŸ“ Methodology



```bash### 1. Feature Selection

# .gitignore å†…å®¹- Started with 47 features in cleaned data

output/                  # æ•´ä¸ªè¾“å‡ºç›®å½•- Xi Chen's LASSO identified 121 important one-hot encoded features

*.pkl                    # æ‰€æœ‰pickleæ–‡ä»¶- Selected top 48 LASSO features (threshold â‰¥ 0.05)

*.parquet               # æ‰€æœ‰parquetæ–‡ä»¶- Mapped to 18 original columns (handles one-hot encoding)

*.log                   # æ—¥å¿—æ–‡ä»¶- See [FEATURE_SELECTION_EXPLANATION.md](./FEATURE_SELECTION_EXPLANATION.md)



# ä¾‹å¤–ï¼šä¿ç•™å°çš„ç´¢å¼•æ–‡ä»¶### 2. Data Preprocessing

!*_index.parquet        # è®­ç»ƒ/éªŒè¯/æµ‹è¯•ç´¢å¼• (æ€»è®¡<5MB)- **Missing Values**: Median for numeric, mode for categorical

!feature_names.txt      # ç‰¹å¾åç§°åˆ—è¡¨- **Categorical Encoding**:

```  - Logistic Regression: One-Hot Encoding (low cardinality only)

  - Random Forest/XGBoost: Label Encoding

**ä¸Šä¼ æ­¥éª¤**:  - LSTM/Transformer: Embedding layers

- **Numeric Features**: StandardScaler normalization

```bash- **Class Imbalance**: Handled via class weights and scale_pos_weight

cd /Users/yuchenzhou/Documents/duke/compsci526/final_proj/proj_v2

### 3. Model Training

# 1. åˆå§‹åŒ–Gitï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰- **Train/Test Split**: 80/20 stratified split

git init- **Validation**: For deep learning models (10% of training)

- **Early Stopping**: Prevents overfitting (patience=5 epochs)

# 2. æ·»åŠ ä»£ç å’Œé…ç½®æ–‡ä»¶- **Evaluation**: ROC-AUC, PR-AUC, Accuracy, Precision, Recall, F1

git add YuchenZhou_jiaqi_Pipeline/.gitignore

git add YuchenZhou_jiaqi_Pipeline/preprocessing/scripts/### 4. Model Architectures

git add YuchenZhou_jiaqi_Pipeline/preprocessing/*.yaml

git add YuchenZhou_jiaqi_Pipeline/preprocessing/*.txt**Traditional ML**:

git add YuchenZhou_jiaqi_Pipeline/preprocessing/*.sh- Logistic Regression: L2 regularization, balanced class weights

git add YuchenZhou_jiaqi_Pipeline/preprocessing/*.md- Random Forest: 100 trees, max_depth=15

git add YuchenZhou_jiaqi_Pipeline/training/- XGBoost: 200 trees, learning_rate=0.1, scale_pos_weight=3.0

git add YuchenZhou_jiaqi_Pipeline/testing/

git add YuchenZhou_jiaqi_Pipeline/*.md**Deep Learning**:

- LSTM: Bidirectional, 2 layers, 128 hidden units

# 3. å¯é€‰ï¼šæ·»åŠ å°çš„ç´¢å¼•æ–‡ä»¶- Transformer: 4 attention heads, 3 encoder layers

git add YuchenZhou_jiaqi_Pipeline/output/*_index.parquet

git add YuchenZhou_jiaqi_Pipeline/output/feature_names.txt---



# 4. æ£€æŸ¥å°†è¦ä¸Šä¼ çš„æ–‡ä»¶## ğŸ¤ Collaboration

git status

This pipeline builds upon collaborative work:

# 5. æäº¤

git commit -m "Add MIMIC-IV preprocessing pipeline and training code"- **Xi Chen**: LASSO feature selection (`XiChen_Lasso/`)

- **Yuchen Zhou**: End-to-end ML pipeline (this folder)

# 6. è¿æ¥è¿œç¨‹ä»“åº“

git remote add origin https://github.com/Yuchennnnnnn/Mimic_iv_readmission_rate_30_days.gitFeature importance coefficients are shared via `Feature_Importance_by_Coef.csv`.



# 7. æ¨é€---

git branch -M main

git push -u origin main## ğŸ“š Key Files

```

| File | Purpose |

**åœ¨READMEä¸­è¯´æ˜æ•°æ®è·å–æ–¹å¼**:|------|---------|

| `config.yaml` | All configuration parameters |

```markdown| `src/train.py` | Main training script (630 lines) |

## ğŸ“¦ æ•°æ®è·å–| `src/feature_selection.py` | LASSO feature integration â­ |

| `src/models.py` | All 5 model implementations |

ç”±äºé¢„å¤„ç†æ•°æ®æ–‡ä»¶è¿‡å¤§ï¼ˆ39GBï¼‰ï¼ŒæœªåŒ…å«åœ¨ä»“åº“ä¸­ã€‚è¯·æŒ‰ä»¥ä¸‹æ–¹å¼è·å–ï¼š| `src/evaluate.py` | Metrics and visualization |

| `reports/metrics.csv` | Complete results table |

### æ–¹å¼1: è‡ªè¡Œè¿è¡Œé¢„å¤„ç†ï¼ˆæ¨èï¼‰| `artifacts/*.pkl` | Trained model files |

bash YuchenZhou_jiaqi_Pipeline/preprocessing/run_all.sh

é¢„è®¡è€—æ—¶: 3-4å°æ—¶---



### æ–¹å¼2: ä¸‹è½½é¢„å¤„ç†æ•°æ®## ğŸ› Troubleshooting

è”ç³»ä½œè€…è·å–äº‘å­˜å‚¨é“¾æ¥ï¼š[your-email]

```### Common Issues



### æ–¹æ¡ˆ2: ä½¿ç”¨Git LFSï¼ˆé€‚åˆ<1GBæ–‡ä»¶ï¼‰**1. NumPy Version Error**

```bash

å¦‚æœä½ åªæƒ³ä¸Šä¼ å°ä¸€ç‚¹çš„æ–‡ä»¶ï¼ˆå¦‚ç´¢å¼•ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨Git LFSï¼š# Error: numpy.dtype size changed

pip install "numpy<2"

```bash```

# å®‰è£…Git LFS

brew install git-lfs       # macOS**2. Missing Packages**

# sudo apt install git-lfs # Linux```bash

pip install -r requirements.txt

# åˆå§‹åŒ–```

git lfs install

**3. CUDA Out of Memory (Deep Learning)**

# è¿½è¸ªç‰¹å®šæ–‡ä»¶ç±»å‹```yaml

git lfs track "output/*_index.parquet"# In config.yaml, reduce batch size:

git lfs track "output/feature_names.txt"deep_learning:

  batch_size: 128  # Instead of 256

# æäº¤.gitattributes```

git add .gitattributes

git commit -m "Configure Git LFS"**4. Feature Mapping Issues**

```bash

# æ­£å¸¸æ·»åŠ å’Œæ¨é€# Check feature mapping:

git add output/*_index.parquetpython check_feature_mapping.py

git push```

```

---

âš ï¸ **æ³¨æ„**: GitHub LFSå…è´¹é¢åº¦ä»…1GBå­˜å‚¨ + 1GBå¸¦å®½/æœˆ

## ğŸ“Š Next Steps

### æ–¹æ¡ˆ3: äº‘å­˜å‚¨é“¾æ¥ï¼ˆæ¨èç”¨äºåˆ†äº«å¤§æ–‡ä»¶ï¼‰

### Immediate Improvements

**æ­¥éª¤**:1. **Hyperparameter Tuning**: Use GridSearchCV for XGBoost

2. **Ensemble Models**: Combine XGBoost + Transformer predictions

1. ä¸Šä¼ æ•°æ®åˆ°äº‘å­˜å‚¨:3. **More Features**: Increase `top_n` to 100 for more features

   - Google Drive4. **Threshold Optimization**: Adjust classification threshold for recall/precision trade-off

   - Dropbox

   - OneDrive### Advanced Enhancements

   - æˆ–å…¶ä»–äº‘å­˜å‚¨æœåŠ¡1. **SHAP Analysis**: Explain individual predictions

2. **Calibration**: Improve probability calibration

2. è·å–åˆ†äº«é“¾æ¥3. **Temporal Validation**: Time-based train/test split

4. **External Validation**: Test on different hospital data

3. åœ¨READMEä¸­æ·»åŠ ä¸‹è½½éƒ¨åˆ†:5. **Clinical Integration**: Deploy as risk calculator API



```markdown---

## ğŸ“¥ ä¸‹è½½é¢„å¤„ç†æ•°æ®

## ğŸ“– References

é¢„å¤„ç†åçš„æ•°æ®ï¼ˆ39GBï¼‰å¯ä»ä»¥ä¸‹é“¾æ¥ä¸‹è½½ï¼š

- **Dataset**: MIMIC-IV Clinical Database v2.0

| æ–‡ä»¶ | å¤§å° | ä¸‹è½½é“¾æ¥ |- **Paper**: Johnson et al. (2023). "MIMIC-IV, a freely accessible electronic health record dataset"

|------|------|---------|- **Methods**: Scikit-learn, XGBoost, PyTorch

| train_data.pkl | 10GB | [Google Drive](https://drive.google.com/...) |- **Evaluation**: Standard ML metrics (Hosmer-Lemeshow, 2013)

| val_data.pkl | 2.9GB | [Google Drive](https://drive.google.com/...) |

| test_data.pkl | 2.4GB | [Google Drive](https://drive.google.com/...) |---

| ç´¢å¼•æ–‡ä»¶ | <5MB | [GitHub Releases](https://github.com/.../releases) |

## ğŸ“§ Contact

### ä¸‹è½½åæ”¾ç½®ä½ç½®ï¼š

å°†ä¸‹è½½çš„æ–‡ä»¶æ”¾åˆ°: `YuchenZhou_jiaqi_Pipeline/output/`**Yuchen Zhou**  

```Duke University - CompSci 526  

Fall 2025  

### éªŒè¯ä¸Šä¼ å†…å®¹

For questions about this pipeline, please refer to the code comments or configuration file.

```bash

# æŸ¥çœ‹å°†è¦æäº¤çš„æ–‡ä»¶---

git status

## ğŸ“œ License

# æŸ¥çœ‹è¢«å¿½ç•¥çš„æ–‡ä»¶

git status --ignoredThis project is for educational purposes as part of CompSci 526 coursework.  

MIMIC-IV data usage follows PhysioNet credentialed access requirements.

# æ£€æŸ¥æ–‡ä»¶å¤§å°

git ls-files | xargs ls -lh---



# æ£€æŸ¥ä»“åº“å¤§å°**Last Updated**: October 2025  

git count-objects -vH**Version**: 1.0  

**Status**: âœ… Production Ready

# ç¡®ä¿æ²¡æœ‰å¤§æ–‡ä»¶
git ls-files | xargs ls -lh | awk '$5 ~ /[0-9]+M/ {print}'
```

### å¦‚æœæ„å¤–æ·»åŠ äº†å¤§æ–‡ä»¶

```bash
# æ–¹æ³•1: ä»æš‚å­˜åŒºç§»é™¤
git reset HEAD path/to/large/file
git rm --cached path/to/large/file

# æ–¹æ³•2: ä»å†å²ä¸­å®Œå…¨ç§»é™¤
git filter-branch --force --index-filter \
  "git rm -rf --cached --ignore-unmatch output/" \
  --prune-empty --tag-name-filter cat -- --all

# æ–¹æ³•3: ä½¿ç”¨BFGå·¥å…·ï¼ˆæ¨èï¼Œæ›´å¿«ï¼‰
# ä¸‹è½½: https://rtyley.github.io/bfg-repo-cleaner/
java -jar bfg.jar --delete-folders output
git reflog expire --expire=now --all
git gc --prune=now --aggressive

# å¼ºåˆ¶æ¨é€ï¼ˆè°¨æ…ä½¿ç”¨ï¼ï¼‰
git push origin --force --all
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1: Parquetè¯»å–é”™è¯¯

**é”™è¯¯ä¿¡æ¯**:
```
ValueError: Wrong number of dimensions. values.ndim > ndim [2 > 1]
ArrowTypeError: Did not pass numpy.dtype object
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„pandasç‰ˆæœ¬
pip install pandas==2.0.3

# 2. åœ¨ä»£ç ä¸­ä½¿ç”¨fastparquetå¼•æ“
import pandas as pd
df = pd.read_parquet('file.parquet', engine='fastparquet')
```

### é—®é¢˜2: å†…å­˜ä¸è¶³

**ç—‡çŠ¶**: 
- è¿›ç¨‹è¢«ç³»ç»Ÿæ€æ­»
- "Killed" é”™è¯¯ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬çš„è„šæœ¬:
   ```bash
   python scripts/step1_load_data_optimized.py  # âœ… ä½¿ç”¨è¿™ä¸ª
   # ä¸è¦ç”¨: python scripts/step1_load_data.py  # âŒ
   ```

2. å‡å°chunk_sizeï¼ˆåœ¨config.yamlä¸­ï¼‰:
   ```yaml
   preprocessing:
     chunk_size: 5000  # ä»10000å‡å°åˆ°5000
   ```

3. å¢åŠ ç³»ç»Ÿswapç©ºé—´ï¼ˆLinux/macOSï¼‰:
   ```bash
   # macOSä¼šè‡ªåŠ¨ç®¡ç†swap
   # Linux:
   sudo fallocate -l 8G /swapfile
   sudo chmod 600 /swapfile
   sudo mkswap /swapfile
   sudo swapon /swapfile
   ```

### é—®é¢˜3: CSVç¼–ç é”™è¯¯

**é”™è¯¯ä¿¡æ¯**:
```
ValueError: Error converting column "gsn" to bytes using encoding UTF8
```

**è§£å†³æ–¹æ¡ˆ**:
è¿™æ˜¯prescriptions.csvçš„ç¼–ç é—®é¢˜ã€‚Prescriptionsæ˜¯å¯é€‰çš„ï¼Œpipelineä¼šè‡ªåŠ¨è·³è¿‡ã€‚

å¦‚æœä½ éœ€è¦prescriptionsæ•°æ®ï¼š
```python
# å°è¯•ä¸åŒçš„ç¼–ç 
df = pd.read_csv('prescriptions.csv', encoding='latin1')
# æˆ–
df = pd.read_csv('prescriptions.csv', encoding='iso-8859-1')
```

### é—®é¢˜4: å¤„ç†é€Ÿåº¦æ…¢

**æ­£å¸¸é€Ÿåº¦å‚è€ƒ**:
- Step 1: 30-40åˆ†é’Ÿ
- Step 2: 5-10åˆ†é’Ÿ
- Step 3: **2-3å°æ—¶** â°ï¼ˆæœ€æ…¢ï¼Œå¤„ç†32ä¸‡+è®°å½•ï¼‰
- Step 4-6: å…±10-15åˆ†é’Ÿ

**ä¼˜åŒ–å»ºè®®**:
1. ä½¿ç”¨SSDè€Œä¸æ˜¯HDD
2. åœ¨åå°è¿è¡Œ: `nohup bash run_all.sh &`
3. Step 3æ˜¯æœ€æ…¢çš„ï¼Œè¿™æ˜¯æ­£å¸¸çš„
4. ç¡®ä¿è¶³å¤Ÿçš„RAMï¼ˆæ¨è32GBï¼‰

### é—®é¢˜5: Gitæ¨é€å¤±è´¥

**é”™è¯¯**: `remote: error: File xxx.pkl is 10.00 GB; this exceeds GitHub's file size limit of 100 MB`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥.gitignoreæ˜¯å¦æ­£ç¡®
cat .gitignore | grep output

# 2. ç§»é™¤å·²æ·»åŠ çš„å¤§æ–‡ä»¶
git rm --cached output/*.pkl
git rm --cached output/*.parquet

# 3. æäº¤æ›´æ”¹
git commit -m "Remove large files"

# 4. æ¨é€
git push
```

### æ£€æŸ¥å’Œç›‘æ§

```bash
# æ£€æŸ¥è¿›åº¦
cd preprocessing
./check_progress.sh

# æŸ¥çœ‹æ—¥å¿—
tail -f full_pipeline.log
tail -f step3_6_full.log

# æŸ¥çœ‹è¾“å‡ºæ–‡ä»¶
ls -lh output/

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep python | grep step

# æŸ¥çœ‹ç³»ç»Ÿèµ„æº
top          # CPUå’Œå†…å­˜
df -h        # ç£ç›˜ç©ºé—´
free -h      # RAMä½¿ç”¨ï¼ˆLinuxï¼‰
```

---

## ğŸ“ˆ æ•°æ®ç»Ÿè®¡

### æœ€ç»ˆæ•°æ®é›†åˆ’åˆ†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ•°æ®é›†      â”‚ æ ·æœ¬æ•°    â”‚ æ¯”ä¾‹       â”‚ å†å…¥é™¢æ•°     â”‚ å†å…¥é™¢ç‡    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è®­ç»ƒé›†      â”‚ 194,672   â”‚ 66.0%      â”‚ 34,438       â”‚ 17.69%      â”‚
â”‚ (2008-2013) â”‚           â”‚            â”‚              â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ éªŒè¯é›†      â”‚ 55,443    â”‚ 18.8%      â”‚ 9,023        â”‚ 16.27%      â”‚
â”‚ (2014-2016) â”‚           â”‚            â”‚              â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æµ‹è¯•é›†      â”‚ 44,790    â”‚ 15.2%      â”‚ 7,208        â”‚ 16.09%      â”‚
â”‚ (2017-2019) â”‚           â”‚            â”‚              â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€»è®¡        â”‚ 294,905   â”‚ 100%       â”‚ 50,669       â”‚ 17.18%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¹´ä»½åˆ†å¸ƒ

```
è®­ç»ƒé›† (2008-2013):
  â”œâ”€ 2008-2010: 128,225 æ ·æœ¬ (65.87%)
  â””â”€ 2011-2013: 66,447 æ ·æœ¬ (34.13%)

éªŒè¯é›† (2014-2016):
  â””â”€ 2014-2016: 55,443 æ ·æœ¬ (100%)

æµ‹è¯•é›† (2017-2019):
  â””â”€ 2017-2019: 44,790 æ ·æœ¬ (100%)
```

### é˜Ÿåˆ—ç­›é€‰æµç¨‹

```
MIMIC-IVåŸå§‹æ•°æ®: 546,028 ä½é™¢è®°å½•
    â†“
ç­›é€‰æ¡ä»¶:
    â”œâ”€ âœ… å¹´é¾„ â‰¥ 18å²
    â”œâ”€ âœ… ä½é™¢æ—¶é•¿ â‰¥ 48å°æ—¶
    â”œâ”€ âœ… æœ‰å‡ºé™¢æ—¶é—´
    â””â”€ âŒ æ’é™¤é™¢å†…æ­»äº¡
    â†“
æœ€ç»ˆé˜Ÿåˆ—: 322,966 ä½é™¢è®°å½• (59.2%)
    â†“
æ•°æ®å®Œæ•´æ€§ç­›é€‰:
    â””â”€ æœ‰å®Œæ•´48å°æ—¶è§‚æµ‹æ•°æ®
    â†“
æœ€ç»ˆè®­ç»ƒæ•°æ®: 294,905 æ ·æœ¬ (91.3%)
```

### ç‰¹å¾ç»Ÿè®¡

```
æ€»ç‰¹å¾æ•°: 49

â”œâ”€ ç”Ÿå‘½ä½“å¾ (19ä¸ª):
â”‚  â”œâ”€ å¿ƒç‡ (heart_rate)
â”‚  â”œâ”€ æ”¶ç¼©å‹ (sbp)
â”‚  â”œâ”€ èˆ’å¼ å‹ (dbp)
â”‚  â”œâ”€ å¹³å‡åŠ¨è„‰å‹ (map)
â”‚  â”œâ”€ ä½“æ¸© (temperature)
â”‚  â”œâ”€ å‘¼å¸é¢‘ç‡ (respiratory_rate)
â”‚  â”œâ”€ è¡€æ°§é¥±å’Œåº¦ (spo2)
â”‚  â”œâ”€ Glasgowæ˜è¿·è¯„åˆ† (gcs_total, gcs_eye, gcs_verbal, gcs_motor)
â”‚  â””â”€ å…¶ä»–ç”Ÿå‘½ä½“å¾...
â”‚
â””â”€ å®éªŒå®¤æŒ‡æ ‡ (30ä¸ª):
   â”œâ”€ è¡€ç³– (glucose)
   â”œâ”€ ç™½ç»†èƒ (wbc)
   â”œâ”€ è¡€çº¢è›‹ç™½ (hemoglobin)
   â”œâ”€ è¡€å°æ¿ (platelets)
   â”œâ”€ è‚Œé… (creatinine)
   â”œâ”€ å°¿ç´ æ°® (bun)
   â”œâ”€ é’  (sodium)
   â”œâ”€ é’¾ (potassium)
   â”œâ”€ æ°¯ (chloride)
   â”œâ”€ ç¢³é…¸æ°¢æ ¹ (bicarbonate)
   â””â”€ å…¶ä»–å®éªŒå®¤æŒ‡æ ‡...

æ³¨: Prescriptions (è¯ç‰©) ç‰¹å¾ç”±äºæ•°æ®é—®é¢˜æœªåŒ…å«
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **MIMIC-IVæ–‡æ¡£**: https://mimic.mit.edu/docs/iv/
- **MIMIC-IVè®¿é—®**: https://physionet.org/content/mimiciv/
- **è®ºæ–‡**: (å¾…è¡¥å……)
- **GitHubä»“åº“**: https://github.com/Yuchennnnnnn/Mimic_iv_readmission_rate_30_days

---

## ğŸ‘¥ è´¡çŒ®è€…

- **Yuchen Zhou** - æ•°æ®é¢„å¤„ç†Pipeline, æ¨¡å‹è®­ç»ƒ
- **Jiaqi** - ç‰¹å¾å·¥ç¨‹, æ¨¡å‹è¯„ä¼°

**è¯¾ç¨‹**: CS526 Machine Learning  
**æœºæ„**: Duke University  
**å­¦æœŸ**: Fall 2025

---

## ğŸ“ License

æœ¬é¡¹ç›®ä½¿ç”¨çš„MIMIC-IVæ•°æ®å—é™äºPhysioNetçš„ä½¿ç”¨åè®®ã€‚

**æ•°æ®è®¿é—®è¦æ±‚**:
1. å®ŒæˆCITIåŸ¹è®­
2. ç­¾ç½²æ•°æ®ä½¿ç”¨åè®®
3. é€šè¿‡PhysioNetç”³è¯·

è¯¦æƒ…: https://physionet.org/content/mimiciv/

---

## â“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š
- æäº¤GitHub Issue
- æˆ–è”ç³»: [your-email]@duke.edu

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢MIT-LCPå›¢é˜Ÿç»´æŠ¤MIMIC-IVæ•°æ®é›†ã€‚

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ11æ—¥
