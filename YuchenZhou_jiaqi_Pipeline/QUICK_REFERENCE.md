# å¿«é€Ÿå‚è€ƒ - Yuchen Zhouçš„30å¤©å†å…¥é™¢é¢„æµ‹Pipeline# ğŸš€ QUICK REFERENCE CARD



## ğŸ¯ æœ€é‡è¦çš„ç»“æœ## Installation & Setup (One Time)

```bash

| æŒ‡æ ‡ | æœ€ä½³æ¨¡å‹ | åˆ†æ•° |cd training

|------|---------|------|pip install -r requirements.txt

| **ROC-AUC** (æ¨èæŒ‡æ ‡) | Transformer | 0.7056 |```

| **F1-Score** (å¹³è¡¡æŒ‡æ ‡) | XGBoost | 0.4947 |

| **Recall** (æ•è·ç‡) | XGBoost | 68.5% |## Quick Test (2 minutes)

```bash

**â­ æ¨èç”Ÿäº§ä½¿ç”¨**: **XGBoost** (æœ€ä½³å¹³è¡¡æ€§èƒ½)cd training

python quick_start.py

---```



## ğŸ“Š æ‰€æœ‰æ¨¡å‹å¯¹æ¯”## Train Models



| æ¨¡å‹ | ROC-AUC | Recall | F1 | è®­ç»ƒæ—¶é—´ |### Train All Models

|------|---------|--------|-----|---------|```bash

| **XGBoost** â­ | 0.7040 | 68.5% | 0.4947 | ~8åˆ†é’Ÿ |cd training

| **Transformer** | 0.7056 | 12.6% | 0.2104 | ~45åˆ†é’Ÿ |python src/train.py --model all --config config.yaml

| **LSTM** | 0.7030 | 14.5% | 0.2354 | ~40åˆ†é’Ÿ |```

| **Random Forest** | 0.6941 | 62.6% | 0.4834 | ~5åˆ†é’Ÿ |

| **Logistic Reg** | 0.6626 | 66.2% | 0.4643 | ~2åˆ†é’Ÿ |### Train Individual Models

```bash

---# Fastest (1 min)

python src/train.py --model logistic

## ğŸš€ ä¸€é”®è¿è¡Œå‘½ä»¤

# Best performance (5 min)

### å¿«é€Ÿè®­ç»ƒï¼ˆæ¨èï¼‰python src/train.py --model xgb

```bash

cd YuchenZhou_Pipeline/training# With custom epochs

./quick_train.shpython src/train.py --model lstm --epochs 30 --batch-size 64

# é€‰æ‹© 2 (è®­ç»ƒLR + RF + XGBoostï¼Œ15åˆ†é’Ÿ)```

```

## View Results

### å•ç‹¬è®­ç»ƒæœ€ä½³æ¨¡å‹```bash

```bash# Metrics table

cd YuchenZhou_Pipeline/trainingcat reports/metrics.csv

python src/train.py --model xgb --config config.yaml

```# Open plots (macOS)

open reports/model_comparison.png

### æŸ¥çœ‹ç»“æœopen reports/roc_curve_xgb.png

```bash

cd YuchenZhou_Pipeline/training/reports# Open plots (Linux)

cat metrics.csvxdg-open reports/model_comparison.png

open roc_curve_xgb.png```

```

## Make Predictions

---```bash

cd testing

## ğŸ“ å…³é”®æ–‡ä»¶ä½ç½®python src/inference.py \

  --model-path ../training/artifacts/xgb.pkl \

```  --preprocessor-path ../training/artifacts/xgb_preprocessor.joblib \

YuchenZhou_Pipeline/  --input new_data.csv \

â”œâ”€â”€ README.md                          # å®Œæ•´æ–‡æ¡£  --output predictions.csv \

â”œâ”€â”€ FEATURE_SELECTION_EXPLANATION.md   # ä¸ºä»€ä¹ˆ18ä¸ªç‰¹å¾ï¼Ÿ  --model-type sklearn

â”œâ”€â”€ training/```

â”‚   â”œâ”€â”€ artifacts/xgb.pkl             # â­ æœ€ä½³æ¨¡å‹

â”‚   â”œâ”€â”€ reports/metrics.csv           # æ‰€æœ‰ç»“æœ## Configuration

â”‚   â””â”€â”€ config.yaml                   # é…ç½®æ–‡ä»¶

â””â”€â”€ Feature_Importance_by_Coef.csv    # LASSOç‰¹å¾é‡è¦æ€§### Update Data Path

``````yaml

# In config.yaml

---data:

  input_path: "../cleaned_data.csv"  # Your data here

## ğŸ’¡ å…³é”®å‘ç°```



### 1. ç‰¹å¾é€‰æ‹©æ•ˆæœ### Adjust Hyperparameters

- **åŸå§‹**: 47åˆ—```yaml

- **LASSOç­›é€‰**: 48ä¸ªé‡è¦ç‰¹å¾ï¼ˆOne-Hotç¼–ç åï¼‰# For imbalanced data

- **æ˜ å°„å›åŸå§‹**: 18åˆ—hyperparameters:

- **æ•ˆæœ**: é™ç»´61.7%ï¼Œæ€§èƒ½ä¿æŒä¼˜ç§€  xgb:

    scale_pos_weight: 5.0  # Higher for more imbalance

### 2. æ¨¡å‹é€‰æ‹©å»ºè®®

# For faster training

**å¦‚æœéœ€è¦...**  lstm:

- **æœ€é«˜AUC**: ç”¨Transformer (0.7056)    num_epochs: 20      # Reduce from 50

- **æœ€å¥½å¹³è¡¡**: ç”¨XGBoost (F1=0.49, Recall=68%)  â­    batch_size: 128     # Increase from 64

- **å¿«é€ŸåŸå‹**: ç”¨Logistic Regression (~2åˆ†é’Ÿ)```

- **å¯è§£é‡Šæ€§**: ç”¨Logistic Regression (ç³»æ•°æ¸…æ™°)

- **ä½è¯¯æŠ¥**: ç”¨Transformer (Precision=65%)## Troubleshooting



**ç”Ÿäº§ç¯å¢ƒ**: **XGBoostæ˜¯æœ€ä½³é€‰æ‹©ï¼**### CUDA Out of Memory

```yaml

### 3. Top 5æœ€é‡è¦ç‰¹å¾# Reduce batch size in config.yaml

hyperparameters:

1. **died_in_hospital** (0.603) - é™¢å†…æ­»äº¡  lstm:

2. **last_service_OMED** (0.451) - æœåŠ¡ç±»å‹    batch_size: 32

3. **gender** (0.382) - æ€§åˆ«```

4. **admission_type** (0.340) - å…¥é™¢ç±»å‹

5. **discharge_location** (0.318) - å‡ºé™¢åœ°ç‚¹### Import Errors

```bash

---# Run from training directory

cd training

## ğŸ”§ å¿«é€Ÿè°ƒæ•´python src/train.py ...

```

### å¢åŠ æ›´å¤šç‰¹å¾

```yaml### Slow Training

# ç¼–è¾‘ training/config.yaml```bash

feature_selection:# Use CPU only

  top_n: 100  # æ”¹ä¸º100ï¼ˆå½“å‰50ï¼‰export CUDA_VISIBLE_DEVICES=""

```

# Or select GPU

### è°ƒæ•´æ¨¡å‹å‚æ•°export CUDA_VISIBLE_DEVICES=0

```yaml```

# XGBoostè¶…å‚æ•°

models:## File Locations

  xgb:

    n_estimators: 300      # æ›´å¤šæ ‘```

    max_depth: 7           # æ›´æ·±çš„æ ‘training/

    learning_rate: 0.05    # æ›´å°å­¦ä¹ ç‡â”œâ”€â”€ src/train.py           â† Main training script

```â”œâ”€â”€ config.yaml            â† Configuration

â”œâ”€â”€ artifacts/             â† Saved models

---â”œâ”€â”€ reports/               â† Results & plots

â””â”€â”€ README.md              â† Full documentation

## ğŸ“ˆ æ€§èƒ½è§£è¯»

testing/

### XGBoostæ··æ·†çŸ©é˜µâ”œâ”€â”€ src/inference.py       â† Prediction script

```â””â”€â”€ artifacts/             â† Copy models here

å®é™… \ é¢„æµ‹    ä¸å†å…¥é™¢    å†å…¥é™¢```

ä¸å†å…¥é™¢       18,244     11,944

å†å…¥é™¢         3,465      7,543## Output Files

```

### After Training

**å«ä¹‰**:```

- âœ… æ­£ç¡®è¯†åˆ«7,543ä¸ªä¼šå†å…¥é™¢çš„æ‚£è€…artifacts/lr.pkl                    # Models

- âŒ æ¼æ‰3,465ä¸ªä¼šå†å…¥é™¢çš„æ‚£è€… (31.5%)reports/metrics.csv                 # All metrics

- âš ï¸ è¯¯æŠ¥11,944ä¸ªä¸ä¼šå†å…¥é™¢çš„æ‚£è€…reports/model_comparison.png        # Comparison chart

reports/roc_curve_*.png            # ROC curves

**é€‚ç”¨åœºæ™¯**: åŒ»é™¢é¢„é˜²æ€§å¹²é¢„ï¼ˆå®å¯è¯¯æŠ¥ä¹Ÿä¸æ¼æŠ¥ï¼‰reports/predictions_*.csv          # Predictions

```

---

## Key Commands Summary

## ğŸ“ è®ºæ–‡å†™ä½œè¦ç‚¹

| Action | Command |

### æ–¹æ³•æè¿°|--------|---------|

```| Quick test | `python quick_start.py` |

æˆ‘ä»¬ä½¿ç”¨MIMIC-IVæ•°æ®é›†çš„205,980ä¸ªä½é™¢è®°å½•ï¼Œ| Train best model | `python src/train.py --model xgb` |

é€šè¿‡LASSOç‰¹å¾é€‰æ‹©ä»47ä¸ªåŸå§‹ç‰¹å¾ä¸­ç­›é€‰å‡º18ä¸ª| Train all | `python src/train.py --model all` |

å…³é”®ç‰¹å¾ã€‚è®­ç»ƒäº†5ä¸ªæ¨¡å‹ï¼ˆLR, RF, XGBoost, | View metrics | `cat reports/metrics.csv` |

LSTM, Transformerï¼‰ï¼Œä½¿ç”¨80/20åˆ†å±‚åˆ’åˆ†ã€‚| Make predictions | `python ../testing/src/inference.py ...` |

XGBoostå–å¾—æœ€ä½³ç»¼åˆæ€§èƒ½ï¼ˆAUC=0.7040, | Run tests | `pytest tests/ -v` |

Recall=68.5%, F1=0.4947ï¼‰ã€‚

```## Model Comparison Quick Reference



### ç»“æœæè¿°| Model | Speed | Performance | Use When |

```|-------|-------|-------------|----------|

æ‰€æœ‰æ¨¡å‹AUCå‡è¶…è¿‡0.66ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆLSTMã€| **Logistic** | âš¡âš¡âš¡ | â­â­ | Need interpretability |

Transformerï¼‰è¾¾åˆ°0.70ä»¥ä¸Šã€‚XGBooståœ¨å¬å›ç‡| **Random Forest** | âš¡âš¡ | â­â­â­ | Want feature importance |

å’ŒF1åˆ†æ•°ä¸Šè¡¨ç°æœ€ä½³ï¼Œé€‚åˆä¸´åºŠåº”ç”¨ã€‚ç‰¹å¾é‡è¦æ€§| **XGBoost** | âš¡âš¡ | â­â­â­â­ | **Best overall** |

åˆ†ææ˜¾ç¤ºé™¢å†…æ­»äº¡ã€æœåŠ¡ç±»å‹ã€æ€§åˆ«æ˜¯æœ€å…³é”®çš„| **LSTM** | âš¡ | â­â­â­ | Sequential patterns |

é¢„æµ‹å› å­ã€‚| **Transformer** | âš¡ | â­â­â­ | Feature interactions |

```

## Typical Performance (MIMIC-IV)

---

```

## ğŸ” å¸¸è§é—®é¢˜Model            Time     ROC-AUC  F1-Score

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

**Q: ä¸ºä»€ä¹ˆé€‰äº†50ä¸ªLASSOç‰¹å¾åªæœ‰18åˆ—ï¼Ÿ**  Logistic         1 min    0.67     0.38

A: LASSOåœ¨One-Hotç¼–ç æ•°æ®ä¸Šè®­ç»ƒï¼Œå¤šä¸ªç¼–ç ç‰¹å¾ï¼ˆå¦‚gender_F, gender_Mï¼‰å¯¹åº”åŒä¸€ä¸ªåŸå§‹åˆ—ï¼ˆgenderï¼‰ã€‚è¯¦è§ [FEATURE_SELECTION_EXPLANATION.md](./FEATURE_SELECTION_EXPLANATION.md)Random Forest    5 min    0.71     0.41

XGBoost          5 min    0.73     0.43  â­

**Q: å“ªä¸ªæ¨¡å‹æœ€å¥½ï¼Ÿ**  LSTM            30 min    0.71     0.42

A: çœ‹ç”¨é€”ï¼šTransformer     40 min    0.72     0.42

- **ä¸´åºŠåº”ç”¨**: XGBoostï¼ˆé«˜recallï¼Œæ•è·68%å†å…¥é™¢ï¼‰```

- **é£é™©æ’åº**: Transformerï¼ˆæœ€é«˜AUCï¼‰

- **å¿«é€Ÿéƒ¨ç½²**: Logistic Regressionï¼ˆè®­ç»ƒå¿«ï¼‰## Support



**Q: å¦‚ä½•æ”¹è¿›ç»“æœï¼Ÿ**  ğŸ“– Full docs: `PIPELINE_README.md`  

A: ğŸ“– Training guide: `training/README.md`  

1. è¶…å‚æ•°è°ƒä¼˜ï¼ˆGridSearchCVï¼‰ğŸ“– Summary: `IMPLEMENTATION_SUMMARY.md`  

2. å¢åŠ æ›´å¤šç‰¹å¾ï¼ˆtop_n: 100ï¼‰

3. é›†æˆå­¦ä¹ ï¼ˆç»„åˆå¤šä¸ªæ¨¡å‹ï¼‰---

4. è°ƒæ•´é˜ˆå€¼ï¼ˆä¼˜åŒ–recall/precisionï¼‰*Keep this file handy for quick reference!*


---

## ğŸ“ å¿«é€Ÿå¸®åŠ©

**æ–‡ä»¶ä¸¢å¤±äº†ï¼Ÿ**
```bash
cd YuchenZhou_Pipeline/training
python src/train.py --model xgb --config config.yaml
# ä¼šé‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶
```

**ç¯å¢ƒæœ‰é—®é¢˜ï¼Ÿ**
```bash
pip install "numpy<2" pandas scikit-learn xgboost matplotlib seaborn pyyaml
```

**æƒ³çœ‹ç‰¹å¾æ˜ å°„ï¼Ÿ**
```bash
python check_feature_mapping.py
```

---

## ğŸ“š æ›´å¤šæ–‡æ¡£

- **å®Œæ•´README**: [README.md](./README.md)
- **ç‰¹å¾é€‰æ‹©è§£é‡Š**: [FEATURE_SELECTION_EXPLANATION.md](./FEATURE_SELECTION_EXPLANATION.md)
- **è®­ç»ƒä»£ç **: [training/src/train.py](./training/src/train.py)
- **é…ç½®æ–‡ä»¶**: [training/config.yaml](./training/config.yaml)

---

**æœ€åæ›´æ–°**: 2025å¹´10æœˆ  
**çŠ¶æ€**: âœ… æ‰€æœ‰æ¨¡å‹å·²è®­ç»ƒå®Œæˆ  
**æ•°æ®**: 205,980æ ·æœ¬ï¼Œ18ç‰¹å¾ï¼Œ26.72%å†å…¥é™¢ç‡
