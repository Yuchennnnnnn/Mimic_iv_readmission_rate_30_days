# ğŸ¥ 30-Day Hospital Readmission Prediction - Complete ML Pipeline

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**End-to-end reproducible machine learning pipeline for predicting 30-day hospital readmissions using MIMIC-IV data.**

---

## ğŸ“‘ Table of Contents

1. [Overview](#overview)
2. [Project Structure](#project-structure)
3. [Quick Start](#quick-start)
4. [Models Implemented](#models-implemented)
5. [Pipeline Architecture](#pipeline-architecture)
6. [Results & Benchmarks](#results--benchmarks)
7. [Documentation](#documentation)
8. [Contributing](#contributing)

---

## ğŸ¯ Overview

This project implements a complete machine learning pipeline to predict whether a patient will be readmitted to the hospital within 30 days of discharge. It includes:

- **5 ML Models**: Logistic Regression, Random Forest, XGBoost, LSTM, Transformer
- **Comprehensive Preprocessing**: Model-specific feature engineering and encoding
- **Full Evaluation Suite**: ROC-AUC, PR-AUC, F1, calibration curves, feature importance
- **Production Ready**: Inference scripts, batch prediction, model serving templates
- **Reproducible**: Fixed seeds, deterministic algorithms, saved artifacts

**Key Features:**
- âœ… Handles imbalanced data with class weighting
- âœ… Supports high-cardinality categorical features
- âœ… Deep learning models with embeddings and attention
- âœ… Comprehensive visualization and reporting
- âœ… Unit tests and integration tests
- âœ… Easy configuration via YAML

---

## ğŸ“‚ Project Structure

```
proj_v2/
â”œâ”€â”€ preprocessing/                    # Data cleaning & feature generation
â”‚   â”œâ”€â”€ generate_readmission_features_step1.py
â”‚   â”œâ”€â”€ generate_readmission_features_step2.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ XiChen_Lasso/                    # LASSO feature selection (baseline)
â”‚   â”œâ”€â”€ 2025Fall_CS526_GroupProject_Lasso.ipynb
â”‚   â””â”€â”€ readmission-lasso-readme.md
â”‚
â”œâ”€â”€ training/                        # ğŸš€ MAIN PIPELINE (this is what you need!)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ preprocess.py          # Data loading & transformations
â”‚   â”‚   â”œâ”€â”€ models.py              # Model definitions
â”‚   â”‚   â”œâ”€â”€ train.py               # Training CLI script
â”‚   â”‚   â”œâ”€â”€ evaluate.py            # Metrics & visualization
â”‚   â”‚   â”œâ”€â”€ dataset.py             # PyTorch datasets
â”‚   â”‚   â””â”€â”€ utils.py               # Helper functions
â”‚   â”œâ”€â”€ artifacts/                 # Saved models & encoders
â”‚   â”œâ”€â”€ reports/                   # Evaluation results
â”‚   â”œâ”€â”€ tests/                     # Unit & integration tests
â”‚   â”œâ”€â”€ config.yaml                # Configuration
â”‚   â”œâ”€â”€ requirements.txt           # Dependencies
â”‚   â”œâ”€â”€ quick_start.py            # Quick test script
â”‚   â””â”€â”€ README.md                  # Detailed documentation
â”‚
â”œâ”€â”€ testing/                       # Inference & deployment
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ inference.py          # Batch prediction script
â”‚   â”œâ”€â”€ artifacts/                # Copied trained models
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ cleaned_data.csv              # Final preprocessed dataset
â””â”€â”€ README.md                     # This file
```

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.9 or higher
python --version

# Clone or navigate to the project
cd proj_v2/training
```

### Install Dependencies

```bash
pip install -r requirements.txt
```

**Main packages:**
- pandas, numpy, scikit-learn, xgboost
- torch (PyTorch for deep learning)
- matplotlib, seaborn (visualization)

### Run Quick Test

```bash
# Generate synthetic data and train a quick model
python quick_start.py
```

This will:
1. Generate synthetic test data (500 samples)
2. Train a Logistic Regression model
3. Evaluate and save results
4. Show where to find outputs

### Train on Real Data

```bash
# Update config.yaml to point to cleaned_data.csv
# Then train all models
python src/train.py --model all --config config.yaml

# Or train individual models
python src/train.py --model logistic --config config.yaml
python src/train.py --model rf --config config.yaml
python src/train.py --model xgb --config config.yaml
python src/train.py --model lstm --config config.yaml --epochs 30
python src/train.py --model transformer --config config.yaml
```

### View Results

```bash
# Check evaluation metrics
cat reports/metrics.csv

# View plots
open reports/roc_curve_logistic.png
open reports/model_comparison.png
```

---

## ğŸ¤– Models Implemented

| Model | Type | Best For | Training Time |
|-------|------|----------|---------------|
| **Logistic Regression** | Linear | Interpretability, baseline | Fast (~1 min) |
| **Random Forest** | Ensemble | Feature importance, robustness | Medium (~5 min) |
| **XGBoost** | Gradient Boosting | Best tabular performance | Medium (~5 min) |
| **LSTM** | Deep Learning | Sequential patterns | Slow (~30 min) |
| **Transformer** | Deep Learning | Feature interactions | Slow (~40 min) |

### Model Details

**Logistic Regression:**
- L2 regularization
- One-hot encoding for low-cardinality categoricals
- Standard scaling for numerics
- Class balancing for imbalanced data

**Random Forest:**
- 100 trees, max depth 10
- Ordinal encoding for all categoricals
- No scaling needed
- Built-in feature importance

**XGBoost:**
- Gradient boosting with hist method
- Scale_pos_weight for imbalance
- Learning rate 0.1, max depth 6
- L1/L2 regularization

**LSTM:**
- Bidirectional, 2 layers, hidden dim 128
- Embeddings for categorical features
- Early stopping on validation AUC
- Dropout 0.3

**Transformer (TabTransformer):**
- 8 attention heads, 3 layers
- Column embeddings for features
- d_model=128, FFN=512
- Mean pooling over features

---

## ğŸ— Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw MIMIC-IV   â”‚
â”‚     Data        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing  â”‚ (Step 1 & 2)
â”‚  - Aggregation  â”‚
â”‚  - Cleaning     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ cleaned_data.csvâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Training Pipeline            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 1. Load & Validate Data    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 2. Detect Column Types     â”‚    â”‚
â”‚  â”‚    - Categorical           â”‚    â”‚
â”‚  â”‚    - Numeric               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 3. Handle Missing Values   â”‚    â”‚
â”‚  â”‚    - Median for numeric    â”‚    â”‚
â”‚  â”‚    - Mode for categorical  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 4. Train/Test Split        â”‚    â”‚
â”‚  â”‚    (80/20, stratified)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 5. Model-Specific Encoding â”‚    â”‚
â”‚  â”‚    - LR: OHE + Scaling     â”‚    â”‚
â”‚  â”‚    - RF/XGB: Ordinal       â”‚    â”‚
â”‚  â”‚    - DL: Embeddings        â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 6. Train Model             â”‚    â”‚
â”‚  â”‚    - Fit                   â”‚    â”‚
â”‚  â”‚    - Validate              â”‚    â”‚
â”‚  â”‚    - Early Stop (DL)       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚             â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ 7. Evaluate                â”‚    â”‚
â”‚  â”‚    - Metrics               â”‚    â”‚
â”‚  â”‚    - Plots                 â”‚    â”‚
â”‚  â”‚    - Feature Importance    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Artifacts     â”‚
â”‚  - Models       â”‚
â”‚  - Encoders     â”‚
â”‚  - Metrics      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Results & Benchmarks

### Expected Performance on MIMIC-IV

**Dataset Characteristics:**
- ~50,000 hospital admissions
- ~25% readmission rate (imbalanced)
- 100+ features (categorical + numeric)

**Typical Results:**

| Model | ROC-AUC | PR-AUC | F1-Score | Training Time |
|-------|---------|--------|----------|---------------|
| Logistic Reg | 0.67 | 0.32 | 0.38 | 1 min |
| Random Forest | 0.71 | 0.36 | 0.41 | 5 min |
| **XGBoost** | **0.73** | **0.39** | **0.43** | 5 min |
| LSTM | 0.71 | 0.37 | 0.42 | 30 min |
| Transformer | 0.72 | 0.38 | 0.42 | 40 min |

**Key Insights:**
- XGBoost typically achieves best performance on tabular data
- Deep learning models competitive but require more tuning
- Logistic Regression provides interpretable baseline
- All models handle imbalance with appropriate weighting

### Top Predictive Features

From feature importance analysis:
1. **days_since_prev_discharge** - Recent admissions
2. **length_of_stay** - Longer stays â†’ higher risk
3. **num_diagnoses** - Complexity indicator
4. **discharge_location** - SNF/Hospice patterns
5. **admission_type** - Emergency vs. elective
6. **age** - Older patients at higher risk
7. **insurance** - Socioeconomic proxy
8. **died_in_hospital** - Terminal cases
9. **num_transfers** - Care coordination issues
10. **ed_los_hours** - Emergency department strain

---

## ğŸ“š Documentation

### Detailed Guides

- **[Training Pipeline README](training/README.md)** - Complete training documentation
- **[Testing/Inference README](testing/README.md)** - Deployment and prediction
- **[Preprocessing README](preprocessing/README.md)** - Data generation pipeline
- **[LASSO Baseline README](XiChen_Lasso/readmission-lasso-readme.md)** - Feature selection

### Configuration

All settings are in `training/config.yaml`:
- Data paths
- Column definitions
- Model hyperparameters
- Training settings
- Evaluation metrics

### API Reference

Key modules:
- `preprocess.py` - Data loading, encoding, transformation
- `models.py` - Model definitions and factory functions
- `train.py` - Training loop and CLI
- `evaluate.py` - Metrics computation and visualization
- `inference.py` - Prediction on new data

---

## ğŸ§ª Testing

### Run Unit Tests

```bash
cd training
pytest tests/ -v
```

**Test Coverage:**
- âœ… Data loading and validation
- âœ… Preprocessing pipelines
- âœ… Encoder roundtrips
- âœ… Integration test with synthetic data

### Quick Integration Test

```bash
cd training
python quick_start.py
```

---

## ğŸ”§ Troubleshooting

### Common Issues

**1. CUDA out of memory (PyTorch models)**
```yaml
# In config.yaml, reduce batch size:
hyperparameters:
  lstm:
    batch_size: 32  # Was 64
```

**2. High-cardinality categorical features**
```yaml
# Increase threshold or group rare categories:
preprocessing:
  ohe_cardinality_threshold: 20
  rare_category_threshold: 50
```

**3. Imbalanced classes**
```yaml
# Adjust class weights:
hyperparameters:
  xgb:
    scale_pos_weight: 5.0  # Increase for more imbalance
```

**4. Import errors**
```bash
# Ensure you're in the correct directory
cd training
python src/train.py --model logistic
```

---

## ğŸ“ Academic Context

**Course:** CS526 - Machine Learning in Healthcare  
**Institution:** Duke University  
**Semester:** Fall 2025  
**Dataset:** MIMIC-IV v3.1 (de-identified ICU data)

**Learning Objectives:**
- End-to-end ML pipeline development
- Handling healthcare data challenges
- Model comparison and evaluation
- Reproducible research practices
- Clinical deployment considerations

---

## ğŸ“– References

**Dataset:**
- Johnson, A., Bulgarelli, L., Pollard, T., Horng, S., Celi, L. A., & Mark, R. (2023). MIMIC-IV (version 3.1). PhysioNet.

**Methods:**
- Logistic Regression: scikit-learn
- Random Forest: Breiman (2001)
- XGBoost: Chen & Guestrin (2016)
- LSTM: Hochreiter & Schmidhuber (1997)
- TabTransformer: Huang et al. (2020)

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- [ ] Add SHAP/LIME for interpretability
- [ ] Implement time-series models for sequential visits
- [ ] Add automated hyperparameter tuning (Optuna)
- [ ] Create Docker deployment
- [ ] Add model monitoring and drift detection
- [ ] Implement federated learning

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ‘¥ Authors

**Project Team:**
- Xi Chen (LASSO feature selection)
- Yuchen Zhou (Pipeline development)
- [Add other team members]

---

## ğŸ™ Acknowledgments

- MIMIC-IV dataset contributors
- Duke CS526 course staff
- Open-source ML community

---

**For detailed usage instructions, see [`training/README.md`](training/README.md)**

**Questions? Open an issue or contact the team!** ğŸš€
