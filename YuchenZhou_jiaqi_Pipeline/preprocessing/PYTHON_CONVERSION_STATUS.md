# Python-Only Preprocessing Pipeline Conversion Status

## ‚úÖ Completed Changes

### 1. File Renaming
- **Old**: `scripts/step1_run_bigquery.py`
- **New**: `scripts/step1_load_data.py`
- **Status**: ‚úÖ Renamed

### 2. Configuration File Updated
- **File**: `config.yaml`
- **Changes**:
  - ‚ùå Removed `bigquery` section (project_id, dataset)
  - ‚úÖ Added `data_paths` section with local CSV paths:
    ```yaml
    data_paths:
      patients: "../../datasets/mimic-iv-3.1/hosp/patients.csv"
      admissions: "../../datasets/mimic-iv-3.1/hosp/admissions.csv"
      chartevents: "../../datasets/mimic-iv-3.1/icu/chartevents.csv"
      labevents: "../../datasets/mimic-iv-3.1/hosp/labevents.csv"
      prescriptions: "../../datasets/mimic-iv-3.1/hosp/prescriptions.csv"
      output_dir: "./output"
    ```

### 3. Dependencies Updated
- **File**: `requirements.txt`
- **Removed**:
  - `google-cloud-bigquery>=2.30.0`
  - `google-auth>=2.0.0`
  - `google-auth-oauthlib>=0.4.0`
  - `google-auth-httplib2>=0.1.0`
- **Kept**: All pandas/numpy/PyTorch dependencies

### 4. Step 1 Script Converted
- **File**: `scripts/step1_load_data.py`
- **Changes**:
  - ‚ùå Removed BigQuery imports
  - ‚úÖ Added pandas CSV loading with chunked reading (1M rows at a time)
  - ‚úÖ Implemented `create_cohort_with_readmission()` function:
    - Merges `patients` + `admissions`
    - Computes age (excludes < 18 years)
    - Excludes in-hospital deaths
    - Excludes admissions < 48 hours
    - Computes 30-day readmission labels using `groupby` + `shift`
  - ‚úÖ Implemented `extract_events_for_cohort()` function:
    - Filters events to cohort `hadm_id`s
    - Filters to 0-48 hour window post-admission
    - Uses chunked reading for large files (chartevents, labevents)
  - ‚úÖ Rewrote `main()` function:
    - Loads all CSVs with progress bars (tqdm)
    - Creates cohort
    - Extracts events
    - Saves to parquet files

### 5. Shell Script Updated
- **File**: `run_all.sh`
- **Changes**:
  - Updated title: "MIMIC-IV Temporal Preprocessing Pipeline (Pure Python)"
  - Updated Step 1 description: "Loading MIMIC-IV data and creating cohort..." (was "Running BigQuery queries...")
  - Updated command: `python scripts/step1_load_data.py` (was `step1_run_bigquery.py`)

---

## ‚è≥ Remaining Tasks

### 1. Complete Documentation Updates
**Files to update**: `README.md`, `QUICKSTART.md`

**Current status**: README still mentions BigQuery in several sections

**Required changes**:
- Remove all BigQuery authentication steps
- Remove SQL query descriptions
- Update prerequisites section:
  - Remove "Google Cloud Setup"
  - Add "MIMIC-IV CSV download from PhysioNet"
- Update "Step-by-Step Workflow" section (currently describes SQL queries)
- Update code snippets to show pandas operations instead of SQL

**Example needed sections**:
```markdown
## Prerequisites

1. **MIMIC-IV Access**: Download CSV files from PhysioNet
   - Required: patients.csv, admissions.csv, chartevents.csv, labevents.csv, prescriptions.csv
2. **Python**: 3.8+
3. **Storage**: ~50GB for raw CSVs, ~10GB for outputs

## Step 1: Load Data and Create Cohort (Python)

Loads MIMIC-IV CSVs and creates cohort with 30-day readmission labels.

**Script**: `scripts/step1_load_data.py`

**Key Functions**:
- `create_cohort_with_readmission()`: Merges patients + admissions, computes labels
- `extract_events_for_cohort()`: Filters events to 48-hour window

**Outputs**:
- `cohort.parquet`: ~200K admissions with readmit_30d labels
- `chartevents_raw.parquet`, `labevents_raw.parquet`, `prescriptions_raw.parquet`
```

### 2. Delete Obsolete SQL Files
**Directory**: `sql/`

**Files to delete**:
- `01_cohort_definition.sql`
- `02_extract_chartevents.sql`
- `03_extract_labevents.sql`
- `04_extract_prescriptions.sql`

**Reason**: These are no longer used in the Python-only pipeline

### 3. Test Full Pipeline
**Command**: `bash run_all.sh`

**Expected outputs**:
- `output/cohort.parquet` (~200K rows)
- `output/chartevents_raw.parquet` (filtered to cohort)
- `output/labevents_raw.parquet` (filtered to cohort)
- `output/prescriptions_raw.parquet` (filtered to cohort)
- `output/train_data.pkl`, `val_data.pkl`, `test_data.pkl` (final outputs)

**Validation checks**:
1. Cohort size: ~200K admissions (down from 546K raw)
2. Readmission rate: ~26.7%
3. No patient overlap between train/val/test splits
4. Final data shape: `(num_samples, 48, ~120)` for LSTM/Transformer

### 4. Update Step 2 (Optional Enhancement)
**File**: `scripts/step2_clean_units.py`

**Current dependency**: May still reference BigQuery item dictionaries

**Suggested change**: Load item mappings from `d_items.csv` (also part of MIMIC-IV) instead of BigQuery:
```python
# Load item dictionary
d_items = pd.read_csv(config['data_paths']['d_items'])  # Add to config.yaml
item_mapping = dict(zip(d_items['itemid'], d_items['label']))

# Apply standardization
df['feature_name'] = df['itemid'].map(item_mapping)
```

---

## üîç Key Implementation Details

### Memory-Efficient Chunked Reading
Large files like `chartevents.csv` (~3-5GB) are processed in chunks:

```python
chunk_size = 1000000  # 1M rows at a time
chartevents_list = []

for chunk in tqdm(pd.read_csv(filepath, chunksize=chunk_size), desc="Loading chartevents"):
    # Filter to cohort hadm_ids
    chunk_cohort = chunk[chunk['hadm_id'].isin(cohort['hadm_id'])]
    if len(chunk_cohort) > 0:
        chartevents_list.append(chunk_cohort)

chartevents = pd.concat(chartevents_list, ignore_index=True)
```

**Benefits**:
- Handles multi-GB files without OOM errors
- Progress bars with tqdm
- Only keeps relevant rows in memory

### 30-Day Readmission Label Computation
Replaced SQL `LEAD() OVER (PARTITION BY ...)` with pandas:

```python
df = df.sort_values(['subject_id', 'admittime'])
df['next_admittime'] = df.groupby('subject_id')['admittime'].shift(-1)
df['days_to_readmit'] = (df['next_admittime'] - df['dischtime']).dt.days
df['readmit_30d'] = (df['days_to_readmit'] <= 30).astype(int)
```

### 48-Hour Window Extraction
```python
def extract_events_for_cohort(events_df, cohort_df):
    # Merge to get admittime
    merged = events_df.merge(
        cohort_df[['hadm_id', 'admittime']], 
        on='hadm_id', 
        how='inner'
    )
    
    # Compute hours since admission
    merged['hours_since_admit'] = (
        merged['charttime'] - merged['admittime']
    ).dt.total_seconds() / 3600
    
    # Filter 0-48 hours
    return merged[(merged['hours_since_admit'] >= 0) & 
                  (merged['hours_since_admit'] < 48)]
```

---

## üéØ Next Steps for User

1. **Update paths in config.yaml** to match your MIMIC-IV download location
2. **Run full pipeline**: `bash run_all.sh`
3. **Verify outputs** in `output/` directory
4. **(Optional) Clean up documentation** to remove remaining BigQuery references

---

## üìä Expected Performance

| Stage | Runtime | Memory | Output Size |
|-------|---------|--------|-------------|
| Step 1: Load data | ~10-15 min | ~8GB peak | ~500MB parquet |
| Step 2: Clean units | ~2-3 min | ~2GB | ~400MB |
| Step 3: Create timeseries | ~5-10 min | ~4GB | ~1GB pkl |
| Step 4: Compute features | ~5-10 min | ~4GB | ~1.2GB pkl |
| Step 5: Temporal split | ~1 min | ~2GB | 3x pkl files |
| Step 6: Save output | ~2 min | ~1GB | Final files |
| **Total** | **~25-40 min** | **~8GB peak** | **~3-4GB** |

---

## ‚úÖ Conversion Complete

The preprocessing pipeline is now **fully Python-based** and requires:
- ‚úÖ No Google Cloud Platform account
- ‚úÖ No BigQuery access
- ‚úÖ No SQL knowledge
- ‚úÖ Only MIMIC-IV CSV files (downloadable from PhysioNet)

All processing is done with pandas/numpy, making the pipeline:
- More reproducible
- Easier to debug
- Platform-independent
- Accessible to researchers without cloud infrastructure
